{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVMCjEV1IEC7"
   },
   "source": [
    "# CIS 5450 Homework 3: Spark SQL\n",
    "\n",
    "## Due: Monday, October 24, 2022 by 11:59pm ET\n",
    "\n",
    "#### **100 points**\n",
    "\n",
    "Welcome to CIS 5450 Homework 3! In this homework you will gain a mastery of using Spark SQL. Over the next few days you will be using an AWS EMR cluster to use Spark to manipulate the  `basic_stats.json` and `defensive.json` datasets, as well as `superbowl.csv`. Use this [link to access](https://drive.google.com/drive/folders/1yViGPJFB4NppRVXOzIiWvqdhK2BiJbQL?usp=sharing) the materials, though you don't need to now!\n",
    "\n",
    "The goal of the homework will be to create a dataset for a time series model.  Yes, we are training you for the portions of the class coming up in the not-so-distant future!!\n",
    "\n",
    "## Notes \n",
    "Before we begin here are some important notes to keep in mind,\n",
    "\n",
    "\n",
    "1.  This notebook is specifically designed to be run on AWS SageMaker (and not on Google Colab). Please follow the instructions on the [Getting Started with AWS Academy](https://docs.google.com/document/d/1n9kK-FYL5dAptpiub9RfJ3XAJlI5NsGfJlEkiQD8uXA/edit?usp=sharing) guide to set up SageMaker and eventually, EMR. \n",
    "2.   **IMPORTANT!** I said it twice, it's really important. In this homework, we will be using AWS resources. You are given a quota ($100) to use for the entirety of the homework. There is a small chance you will use all this money, however it is important that at the end of every session, you **shut down your EMR cluster**.\n",
    "3.   **Be sure you use SageMaker for this Homework** since we must connect to the EMR cluster and local Jupyter will have issues doing that. Using a SageMaker Notebook with an EMR cluster has two important abnormalities:\n",
    "    * The first line of any cell in which you will use the spark session must be `%%spark`. Notice that all cells below have this.\n",
    "    * You will, unfortunately, not be able to stop a cell while it is running. If you wish to do so, you will need to restart your cluster. See the Setup EMR Document for reference.\n",
    "4.   You are **required** to use Spark SQL queries to handle the data in the assignment. Mastering SQL is more beneficial than being able to use Spark commands (functions) as it will show up in more areas of programming and data science/analytics than just Spark. Use the following [function list](https://spark.apache.org/docs/latest/api/sql/index.html#) to see all the SQL functions avaliable in Spark.\n",
    "5.   Throughout the homework you will be manipulating Spark dataframes (sdfs). We do not specify any ordering on the final output. You are welcome to order your final tables in whatever way you deem fit. We will conduct our own ordering when we grade.\n",
    "6. Based on the challenges you've faced in the previous homework, we are including information on the expected schema of your results.  Apache Spark is very fiddly but we hope this will help.\n",
    "7. There are portions of this homework that are _very_ hard. We urge you start early to come to office hours and get help if you get stuck. But don't worry, I can see the future, and you all got this.\n",
    "\n",
    "With that said, let's dive in!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XEqGpEGBWs5"
   },
   "source": [
    "## Step 0: Set up EMR\n",
    "\n",
    "Follow the [Getting Started with AWS Academy](https://docs.google.com/document/d/1n9kK-FYL5dAptpiub9RfJ3XAJlI5NsGfJlEkiQD8uXA/edit?usp=sharing) guide for setting up the cluster.\n",
    "\n",
    "Move on to Step 0.1 once you have a cluster (and IDE) set up. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iBPXxgAdXkv"
   },
   "source": [
    "### Step 0.1: The Superfluous Setup\n",
    "\n",
    "Run the following two cells. Apart from installing the usual dependencies (such as PennGrader), we will also install [sparkmagic](https://github.com/jupyter-incubator/sparkmagic), which allows your (Jupyter) notebook to connect to and use your EMR Cluster through [Livy](https://livy.incubator.apache.org), a REST service for Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvkEbVaaAQ1e",
    "outputId": "83ee6c01-2055-42ac-bffc-66f567c4278a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: penngrader in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (0.4)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install penngrader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "6WAJmQ8IAbRs",
    "outputId": "05eb920c-e0fe-42ec-f363-0a7328c28eac"
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL6n768EPt9E"
   },
   "source": [
    "### Step 0.2: The Splendid Spark Connection \n",
    "\n",
    "Now, connect your notebook to the EMR cluster you created. In the first cell, copy the link to the Master Public DNS specified in the setup document.\n",
    "\n",
    "For example, if my URL (directly from the AWS CloudFormation's \"Outputs\" tab) is `\thttp://ec2-44-201-74-248.compute-1.amazonaws.com` and my password was `password1`, I'd run: \n",
    "\n",
    "```\n",
    "%spark add -s spark_session -l python -u http://ec2-44-201-74-248.compute-1.amazonaws.com -a cis545-livy -p password1 -t Basic_Access\n",
    "```\n",
    "\n",
    "**TODO**: Substitute the url and your password in the above example line in the cell below \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G9QbylT-jqX9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1666724100803_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-92-121.ec2.internal:20888/proxy/application_1666724100803_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-92-197.ec2.internal:8042/node/containerlogs/container_1666724100803_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Copy the line above, enter your Master Public DNS with the proper formatting and host, and update the password\n",
    "%spark add -s spark_session -l python -u http://ec2-3-94-88-156.compute-1.amazonaws.com -a cis545-livy -p 123456 -t Basic_Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QwKAHhQL0lf7"
   },
   "outputs": [],
   "source": [
    "# If you ever need to restart, you may need to...\n",
    "# %spark delete -s my_session\n",
    "#OR just factory reset runtime under the runtime tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1IQjUwNObb8"
   },
   "source": [
    "**TODO**: Enter your 8-digit Penn Key as an integer in the cell \n",
    "below. This will be used in the autograder.  **Please also update the cell below, with the same ID!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7zM20juwqqQF"
   },
   "outputs": [],
   "source": [
    "from penngrader.grader import *\n",
    "STUDENT_ID = 10760059"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "V8Oo_5D7qoWp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PennGrader initialized with Student ID: 10760059\n",
      "\n",
      "Make sure this correct or we will not be able to store your grade\n"
     ]
    }
   ],
   "source": [
    "grader = PennGrader(homework_id = 'CIS_5450O_Fall22_HW3', student_id = STUDENT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvl5MJg6cLHr"
   },
   "source": [
    "**Please make sure you also update this one, so the grader can similarly be updated on Spark/EMR!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9NQjHLyKcL5H"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PennGrader initialized with Student ID: 10760059\n",
      "\n",
      "Make sure this correct or we will not be able to store your grade"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "from penngrader.grader import *\n",
    "STUDENT_ID =  10760059\n",
    "grader = PennGrader(homework_id = 'CIS_5450O_Fall22_HW3', student_id = STUDENT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfs-EQZUzF4j"
   },
   "source": [
    "Run the above cells to setup the autograder in BOTH the Spark session as well as locally, make sure to have set your 8 digit Penn ID in the cell above. It will also import all the modules you need for the homework.\n",
    "\n",
    "_Note_: Since we are using an EMR cluster we will only have access to some of modules that exist for Python, meaning things like `pandas`, `numpy`, etc. may not all be available. We have written the entire homework such that the solution does not require any of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nyAa0wn1XCD"
   },
   "source": [
    "## Step 1: Data Wrangling, Cleaning, and Shaping\n",
    "\n",
    "It's football fever! We recently witnessed Super Bowl LVI, in which Los Angeles Rams trumped the Cincinnati Bengals to clinch the title at their brand new home stadium. To continue the hype, we thought it would be exciting for you to work on NFL data and that's what we will be doing today.\n",
    "\n",
    "<br>\n",
    "\n",
    "![Winners](https://imagez.tmz.com/image/d6/4by3/2022/02/14/d62fc8c540c348c0aa75730b9c9a5b8d_md.jpg)\n",
    "\n",
    "<br>\n",
    "\n",
    "The data you will use is stored in an AWS S3 bucket, a cloud storage service. You now need to download it onto the nodes of your [EMR cluster](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf_ADEXnIK0b"
   },
   "source": [
    "### Step 1.1: The Stupendous Schema\n",
    "\n",
    "When loading data, Spark will try to infer its structure on its own. This process is faulty because Spark will sometimes infer the type incorrectly. Spark's ability to determine types is not reliable, thus you will need to define a schema for `basic_stats2.json` and `defensive3.json`.\n",
    "\n",
    "A schema is a description of the structure of data. We have given you an example with `defensive3.json` and you will be defining an explicit schema for `basic_stats2.json`. \n",
    "\n",
    "\n",
    "In Spark, schemas are defined using a `StructType` object. This is a collection of data types, termed `StructField`'s, that specify the structure and variable type of each component of the dataset. For example, suppose we have the following simple JSON object:\n",
    "```json\n",
    "{\n",
    " \"student_name\": \"Data Wrangler\",\n",
    " \"GPA\": 1.4,\n",
    " \"courses\": [\n",
    "    {\"department\": \"Computer and Information Science\",\n",
    "     \"course_id\": \"CIS 545\",\n",
    "     \"semester\": \"Fall 2021\"},\n",
    "    {\"department\": \"Computer and Information Science\",\n",
    "     \"course_id\": \"CIS 555\",\n",
    "     \"semester\": \"Fall 2021\"}\n",
    " ],\n",
    " \"grad_year\": 2022\n",
    "}\n",
    "```\n",
    "\n",
    "We would define its schema as follows:\n",
    "\n",
    "```python       \n",
    "schema = StructType([\n",
    "           StructField(\"student_name\", StringType(), nullable=True),\n",
    "           StructField(\"GPA\", FloatType(), nullable=True),\n",
    "           StructField(\"courses\", ArrayType(\n",
    "                StructType([\n",
    "                  StructField(\"department\", StringType(), nullable=True),\n",
    "                  StructField(\"course_id\", StringType(), nullable=True),\n",
    "                  StructField(\"semester\", StringType(), nullable=True)\n",
    "                ])\n",
    "           ), nullable=True),\n",
    "           StructField(\"grad_year\", IntegerType(), nullable=True)\n",
    "         ])\n",
    "```\n",
    "\n",
    "\n",
    "Each `StructField` has the following structure: `(name, type, nullable)`. The `nullable` flag defines that the specified field may be empty. Your first task is to define the `schema` of `basic_stats2.json`. You can take a look at the schema for `defensive3.json` for reference. A smaller version of the JSON dataset can be found here. [defensive3.json](https://drive.google.com/file/d/1B75g3-GEdrXTiWn9069bDwvIia9xQIQR/view?usp=sharing), [basic_stats2.json](https://drive.google.com/file/d/1aF4fnH4JI_r8wWN27qFdyjiXWOs4mlg9/view?usp=sharing).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There is also no grading cell for this step.  But your JSON file won't load if it's wrong, so you have a way of testing.\n",
    "\n",
    "**TODO**: Create a schema for ``basic_stats2.json``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pL-Ps4KWIJ9e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#schema for defensive.json\n",
    "\n",
    "schema_defensive = StructType([\n",
    "                     \n",
    "                     StructField(\"Player Id\", StringType(), nullable = True),\n",
    "                     StructField(\"Name\", StringType(), nullable = True),\n",
    "                     StructField(\"Position\", StringType(), nullable = True),\n",
    "                     StructField(\"Year\", IntegerType(), nullable = True),\n",
    "                     StructField(\"Team\", StringType(), nullable = True),\n",
    "                     StructField(\"Games Played\", StringType(), nullable = True),\n",
    "                     StructField(\"Total Tackles\", StringType(), nullable = True),\n",
    "                     StructField(\"Solo Tackles\", StringType(), nullable = True),\n",
    "                     StructField(\"Assisted Tackles\", StringType(), nullable = True),\n",
    "                     StructField(\"Sacks\", StringType(), nullable = True),\n",
    "                     StructField(\"Safties\", StringType(), nullable = True),\n",
    "                     StructField(\"Passes Defended\", StringType(), nullable = True),\n",
    "                     StructField(\"Ints\", StringType(), nullable = True),\n",
    "                     StructField(\"Ints for TDs\", StringType(), nullable = True),\n",
    "                     StructField(\"Int Yards\", StringType(), nullable = True),\n",
    "                     StructField(\"Yards Per Int\", StringType(), nullable = True),\n",
    "                     StructField(\"Longest Int Return\", StringType(), nullable = True),            \n",
    "                ])\n",
    "\n",
    "\n",
    "#TODO: Create a schema for basic_stats2.json\n",
    "         #TODO: Create a schema for basic_stats2.json\n",
    "schema_basic_stats2 = StructType([\n",
    "                     \n",
    "                     StructField(\"Age\", StringType(), nullable = True),\n",
    "                     StructField(\"Birth Place\", StringType(), nullable = True),\n",
    "                     StructField(\"Birthday\", StringType(), nullable = True),\n",
    "                     StructField(\"College\",StringType(), nullable = True),\n",
    "                     StructField(\"Current Status\", StringType(), nullable = True),\n",
    "                     StructField(\"Current Team\", StringType(), nullable = True),\n",
    "                     StructField(\"Experience\", StringType(), nullable = True),\n",
    "                     StructField(\"Height (inches)\", StringType(), nullable = True),\n",
    "                     StructField(\"High School\", StringType(), nullable = True),\n",
    "                     StructField(\"High School Location\", StringType(), nullable = True),\n",
    "                     StructField(\"Name\", StringType(), nullable = True),\n",
    "                     StructField(\"Number\", StringType(), nullable = True),\n",
    "                     StructField(\"Player Id\", StringType(), nullable = True),\n",
    "                     StructField(\"Position\", StringType(), nullable = True),\n",
    "                     StructField(\"Weight (lbs)\", StringType(), nullable = True),\n",
    "                     StructField(\"Years Played\", StringType(), nullable = True),        \n",
    "                ])           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Su604X9ggc2"
   },
   "source": [
    "### Step 1.2: The Langorous Load\n",
    "\n",
    "In the following cell, load the `basic_stats2.json` dataset and the `defensive3.json` from GCP's storage service into Spark dataframes (sdf) called `basic_stats_sdf` and `defensive_sdf` respectively. If you have constructed `schema1` and `schema2` correctly, `spark.read.json()` will read in the dataset. \n",
    "\n",
    "***You do not need to edit this cell***. If this doesn't work, go back to the prior cell and update your schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ji-KW2sAiB6r"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "defensive_stats_sdf = spark.read.json(\"s3://penn-cis545-files/defensive3.json\", schema=schema_defensive, multiLine = True, primitivesAsString = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nvurIgjLkdZL"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+--------+----+-----------------+------------+-------------+------------+----------------+-----+-------+---------------+----+------------+---------+-------------+------------------+\n",
      "|         Player Id|          Name|Position|Year|             Team|Games Played|Total Tackles|Solo Tackles|Assisted Tackles|Sacks|Safties|Passes Defended|Ints|Ints for TDs|Int Yards|Yards Per Int|Longest Int Return|\n",
      "+------------------+--------------+--------+----+-----------------+------------+-------------+------------+----------------+-----+-------+---------------+----+------------+---------+-------------+------------------+\n",
      "|quinnjohnson/79593|Johnson, Quinn|        |2013| Tennessee Titans|           4|            0|          --|              --|   --|     --|             --|  --|          --|       --|            0|                --|\n",
      "|quinnjohnson/79593|Johnson, Quinn|        |2012| Tennessee Titans|          16|            1|           1|               0|    0|     --|              0|  --|          --|       --|            0|                --|\n",
      "|quinnjohnson/79593|Johnson, Quinn|        |2011| Tennessee Titans|           4|            0|          --|              --|   --|     --|             --|  --|          --|       --|            0|                --|\n",
      "|quinnjohnson/79593|Johnson, Quinn|        |2011|   Denver Broncos|           0|            0|          --|              --|   --|     --|             --|  --|          --|       --|            0|                --|\n",
      "|quinnjohnson/79593|Johnson, Quinn|        |2010|Green Bay Packers|          11|            4|           4|               0|    0|     --|              0|  --|          --|       --|            0|                --|\n",
      "+------------------+--------------+--------+----+-----------------+------------+-------------+------------+----------------+-----+-------+---------------+----+------------+---------+-------------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# Let's print out the first few rows to see how the data looks like in tabular form\n",
    "defensive_stats_sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wI5-LRaC0Os"
   },
   "source": [
    "**TODO**: Create a spark dataframe for ``basic_stats2``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8DIpp8P8NM7t"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "#TODO: Create a spark dataframe for basic_stats2\n",
    "basic_stats_sdf = spark.read.json(\"s3://penn-cis545-files/basic_stats2.json\", schema=schema_basic_stats2, multiLine = True, primitivesAsString = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMVCVotcE1wv"
   },
   "source": [
    "The cell below shows how to run SQL commands on Spark tables. Use this as a template for all your SQL queries in this notebook. \n",
    "\n",
    "***You do not need to edit this cell***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NJSVWeGiEO5c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+---------+--------------------+--------------+--------------------+----------+---------------+--------------------+--------------------+-----------------+------+--------------------+--------+------------+------------+\n",
      "| Age|    Birth Place| Birthday|             College|Current Status|        Current Team|Experience|Height (inches)|         High School|High School Location|             Name|Number|           Player Id|Position|Weight (lbs)|Years Played|\n",
      "+----+---------------+---------+--------------------+--------------+--------------------+----------+---------------+--------------------+--------------------+-----------------+------+--------------------+--------+------------+------------+\n",
      "|  45|   Griffin , GA|10/8/1971|       Florida State|       Retired|                    | 2 Seasons|             73|                    |                    |   Ellison, 'Omar|  null|'omarellison/2500540|        |         200| 1995 - 1996|\n",
      "|  22|Fort Worth , TX|3/21/1995|             Alabama|        Active|       Detroit Lions|2nd season|             75|Arlington Heights HS|      Fort Worth, TX|Robinson, A'Shawn|    91|a'shawnrobinson/2...|      DT|         320|            |\n",
      "|null|               |         |             Unknown|       Retired|                    |  1 Season|             74|                    |                    |      Bauer, A.C.|  null|   a.c.bauer/2509176|        |         210| 1923 - 1923|\n",
      "|  25|    Dallas , TX|8/16/1991|     Central Florida|        Active|Jacksonville Jaguars|5th season|             72|           Tucker HS|                  GA|      Bouye, A.J.|  null|   a.j.bouye/2541162|      CB|         191|            |\n",
      "|  25|   Bamberg , SC|10/3/1991|      South Carolina|        Active|Jacksonville Jaguars|3rd season|             75| Bamberg-Ehrhardt HS|         Bamberg, SC|       Cann, A.J.|    60|    a.j.cann/2552330|      OG|         317|            |\n",
      "|  25|               |2/17/1992|       Robert Morris|       Retired|                    |  1 Season|             76|                    |                    |     Dalton, A.J.|  null|  a.j.dalton/2550553|        |         280| 2014 - 2014|\n",
      "|  34|    Durham , NC|5/29/1983|North Carolina State|       Retired|                    | 2 Seasons|             70|                    |                    |      Davis, A.J.|  null|   a.j.davis/2507194|        |         189| 2007 - 2009|\n",
      "|  27|Birmingham , AL| 7/6/1989|  Jacksonville State|       Retired|                    | 3 Seasons|             72|                    |                    |      Davis, A.J.|  null|   a.j.davis/2534821|        |         183| 2012 - 2014|\n",
      "|  25| Iowa City , IA|9/20/1991|            Arkansas|        Active|      Denver Broncos|3rd season|             77|        Iowa City HS|                  IA|      Derby, A.J.|    83|   a.j.derby/2552580|      TE|         255|            |\n",
      "|  29|    Marion , IN|9/18/1987|                Iowa|       Retired|                    | 5 Seasons|             76|                    |                    |       Edds, A.J.|  null|     a.j.edds/496921|        |         256| 2010 - 2014|\n",
      "+----+---------------+---------+--------------------+--------------+--------------------+----------+---------------+--------------------+--------------------+-----------------+------+--------------------+--------+------------+------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create SQL-accesible table\n",
    "basic_stats_sdf.createOrReplaceTempView(\"basic_stats\")\n",
    "\n",
    "# Declare SQL query to be excecuted\n",
    "query = '''SELECT * \n",
    "           FROM basic_stats ORDER BY `Player Id` LIMIT 10'''\n",
    "\n",
    "# Save the output sdf of spark.sql() as answer_sdf and convert to Pandas\n",
    "answer_basic_sdf = spark.sql(query)\n",
    "answer_basic_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLF-swRBTuZG"
   },
   "source": [
    "We will copy the `answer_sdf` to Colab to submit to PennGrader..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wOyxJ6aORvs6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 4/4 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "## 4 points\n",
    "## AUTOGRADER Step 1.2:   \n",
    "grader.grade(test_case_id = 'first', answer = pd.read_json(answer_basic_sdf.toPandas().to_json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A9nzeVaG4QV"
   },
   "source": [
    "**TODO**: Now, implement the exact same thing yourself for defensive_stats_sdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-TGpKgWrG3DC"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------+----+--------------------+------------+-------------+------------+----------------+-----+-------+---------------+----+------------+---------+-------------+------------------+\n",
      "|           Player Id|             Name|Position|Year|                Team|Games Played|Total Tackles|Solo Tackles|Assisted Tackles|Sacks|Safties|Passes Defended|Ints|Ints for TDs|Int Yards|Yards Per Int|Longest Int Return|\n",
      "+--------------------+-----------------+--------+----+--------------------+------------+-------------+------------+----------------+-----+-------+---------------+----+------------+---------+-------------+------------------+\n",
      "|a'shawnrobinson/2...|Robinson, A'Shawn|      DT|2016|       Detroit Lions|          16|           30|          22|               8|    2|     --|              7|  --|          --|       --|            0|                --|\n",
      "|   a.j.bouye/2541162|      Bouye, A.J.|      CB|2016|      Houston Texans|          15|           63|          48|              15|    1|     --|             16|   1|           0|        0|            0|                 0|\n",
      "|   a.j.bouye/2541162|      Bouye, A.J.|      CB|2015|      Houston Texans|          15|           16|          15|               1|    0|     --|              6|   2|           0|        9|          4.5|                 9|\n",
      "|   a.j.bouye/2541162|      Bouye, A.J.|      CB|2014|      Houston Texans|          14|           59|          52|               7|    0|      0|             10|   3|           1|      120|           40|                67|\n",
      "|   a.j.bouye/2541162|      Bouye, A.J.|      CB|2013|      Houston Texans|           6|            2|           2|               0|    0|     --|              0|  --|          --|       --|            0|                --|\n",
      "|     a.j.edds/496921|       Edds, A.J.|        |2014|       New York Jets|           4|            1|           1|               0|    0|     --|              0|  --|          --|       --|            0|                --|\n",
      "|     a.j.edds/496921|       Edds, A.J.|        |2011|  Indianapolis Colts|           9|           12|           2|              10|    0|     --|              0|  --|          --|       --|            0|                --|\n",
      "|     a.j.edds/496921|       Edds, A.J.|        |2011|New England Patriots|           2|            1|           1|               0|    0|     --|              0|  --|          --|       --|            0|                --|\n",
      "|     a.j.edds/496921|       Edds, A.J.|        |2014|Jacksonville Jaguars|           2|            0|          --|              --|   --|     --|             --|  --|          --|       --|            0|                --|\n",
      "|  a.j.feeley/2504566|     Feeley, A.J.|        |2011|      St. Louis Rams|           5|            0|          --|              --|   --|     --|             --|  --|          --|       --|            0|                --|\n",
      "+--------------------+-----------------+--------+----+--------------------+------------+-------------+------------+----------------+-----+-------+---------------+----+------------+---------+-------------+------------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# TODO: Create SQL-accesible table\n",
    "\n",
    "defensive_stats_sdf.createOrReplaceTempView(\"defensive_stats\")\n",
    "\n",
    "#TODO: Declare SQL query to be excecuted\n",
    "\n",
    "query = '''SELECT * \n",
    "           FROM defensive_stats ORDER BY `Player Id` LIMIT 10'''\n",
    "#TODO: Save the output sdf of spark.sql() as answer_defensive_sdf and convert to Pandas\n",
    "\n",
    "answer_defensive_sdf = spark.sql(query)\n",
    "answer_defensive_sdf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sfVITUBotr73"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 4/4 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "## 4 points\n",
    "\n",
    "## AUTOGRADER Step 1.2:   \n",
    "grader.grade(test_case_id = 'second', answer = pd.read_json(answer_defensive_sdf.toPandas().to_json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "705XndyQYW6f"
   },
   "source": [
    "**TODO**: In the next cell, create `step_1_2_sdf` to fetch the data from the above basic_stats table, returning rows with schema `(Player Id, Name)`, in **lexicographical order** of `Name`.  Limit your sdf to 10 rows. Save your final answer to Colab to submit to PennGrader, as demonstrated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zOpHsWR2qQAY"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|           Player Id|             Name|\n",
      "+--------------------+-----------------+\n",
      "|isaakoaaitui/2531731|   Aaitui, Isaako|\n",
      "|   jonabbate/2495524|      Abbate, Jon|\n",
      "|    joeabbey/2508144|       Abbey, Joe|\n",
      "|  fayeabbott/2508147|     Abbott, Faye|\n",
      "| vinceabbott/2508148|    Abbott, Vince|\n",
      "|jaredabbrederis/2...|Abbrederis, Jared|\n",
      "|dukeabbruzzi/2508149|   Abbruzzi, Duke|\n",
      "| naderabdallah/89680|  Abdallah, Nader|\n",
      "|mehdiabdesmad/255...|  Abdesmad, Mehdi|\n",
      "|isaabdul-quddus/2...|Abdul-Quddus, Isa|\n",
      "+--------------------+-----------------+"
     ]
    }
   ],
   "source": [
    "%%spark \n",
    "\n",
    "# TODO: create step_1_2_sdf\n",
    "query = '''SELECT `Player Id`, `Name`\n",
    "           FROM basic_stats \n",
    "           ORDER BY `Name` LIMIT 10'''\n",
    "\n",
    "step_1_2_sdf = spark.sql(query)\n",
    "step_1_2_sdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "p4hU1ihWYjL6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 5/5 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# 5 points\n",
    "grader.grade(test_case_id = 'lex_10_ids_last_names', answer = step_1_2_sdf.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2vq-3ZfBYZD"
   },
   "source": [
    "### Step 1.3: Further Cleaning\n",
    "\n",
    "**TODO**: Now, we can see that our data is a little ill-formatted, so let us do some cleaning for our two dataframes. Here is what you will do:\n",
    "\n",
    "For Defensive:\n",
    "\n",
    "1. Drop all the rows that have null values in Total Tackles and Games Played.\n",
    "2. We only want to consider data post 2000 (inclusive).\n",
    "\n",
    "\n",
    "Save this in `defensive_cleaned_sdf`.\n",
    "\n",
    "Once you have done that, we want to select the basic stats only for those filtered players in `defensive_cleaned_sdf` and create a cumulated dataframe called `defensive_player_stats_sdf`.\n",
    "\n",
    "For defensive_player_stats, you need attributes from both `basic_stats` and `defensive_stats`. \n",
    "\n",
    "- It may be helpful to check out USING() in your JOIN instead of ON. Read about the differences [here](https://www.neilwithdata.com/join-using)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "0G7tYJOJDp1q"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "#TODO: Create defensive_cleaned_sdf\n",
    "defensive_stats_sdf.createOrReplaceTempView(\"defensive_stats\")\n",
    "\n",
    "\n",
    "query = '''SELECT *\n",
    "           FROM defensive_stats\n",
    "           '''\n",
    "#TODO: Save the output sdf of spark.sql() as answer_defensive_sdf and convert to Pandas\n",
    "\n",
    "defensive_sdf = spark.sql(query)\n",
    "defensive_cleaned_sdf = defensive_sdf[defensive_sdf['Total Tackles']!='null']\n",
    "defensive_cleaned_sdf = defensive_cleaned_sdf[defensive_cleaned_sdf['Games Played']!='null']\n",
    "#defensive_cleaned_sdf \n",
    "#defensive_cleaned_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "nqflA81xVKoo"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 4/4 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# 4 points\n",
    "grader.grade(test_case_id = 'defensive_cleaned', answer = defensive_cleaned_sdf.toPandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "Ot-WaWfYNO7t"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "#TODO: Create defensive_player_stats_sdf\n",
    "defensive_cleaned_sdf.createOrReplaceTempView(\"defensive_cleaned\")\n",
    "basic_stats_sdf.createOrReplaceTempView(\"basic_stats\")\n",
    "\n",
    "query = '''SELECT *\n",
    "           FROM defensive_cleaned as d\n",
    "           inner join basic_stats as b using (`Player Id`)\n",
    "           '''\n",
    "#TODO: Save the output sdf of spark.sql() as answer_defensive_sdf and convert to Pandas\n",
    "\n",
    "defensive_player_stats_sdf = spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "1HO4WgzDXhql"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 4/4 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# 4 points\n",
    "grader.grade(test_case_id = 'defensive_player_stats', answer = defensive_player_stats_sdf.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsLea-1Tyu0v"
   },
   "source": [
    "## Step 2: Analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svOO4iLPist4"
   },
   "source": [
    "### Step 2.1: The Robust Ratio\n",
    "\n",
    "Now, we are interested in seeing the player with the highest total tackles / games played ratio from each team for each year and their corresponding experience. This involves derieving data from both the dataframes we created just now.\n",
    "\n",
    "Your task is to first find the player with the highest ratio for each team per year from the defensive_player_stats_df, and then join that with the basic_stats_df to find the experience of that player.\n",
    "\n",
    "**TODO**: Create an sdf called `best_tackles_sdf` that contains the above information. Then, make sure to only retain data pertaining to Team, player Id, Year, tackle_ratio, and experience. \n",
    "\n",
    "Remember we will sort the dataframe when grading so you can sort the elements however you wish (you don't need to if you don't want to).\n",
    "\n",
    "You should have the following columns in your answer:`Experience, ID, Year, Ratio, Team`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "Kt16tyP0klQX"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# TODO: Create best_tackles_sdf\n",
    "defensive_player_stats_sdf.createOrReplaceTempView(\"defensive_player_stats\")\n",
    "\n",
    "\n",
    "query = '''WITH t1 as (SELECT `Experience`,`Player Id` as ID, `Year`, `Total Tackles`/`Games Played` as Ratio, `Team`\n",
    "           FROM defensive_player_stats)\n",
    "           Select Experience, ID,Year, Ratio, Team\n",
    "           From t1\n",
    "\n",
    "           '''\n",
    "#TODO: Save the output sdf of spark.sql() as answer_defensive_sdf and convert to Pandas\n",
    "\n",
    "best_tackles_sdf = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "-a3xLDEYCcwl"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----+------+----------------+\n",
      "|Experience|                ID|Year| Ratio|            Team|\n",
      "+----------+------------------+----+------+----------------+\n",
      "| 5 Seasons|quinnjohnson/79593|2013|   0.0|Tennessee Titans|\n",
      "| 5 Seasons|quinnjohnson/79593|2012|0.0625|Tennessee Titans|\n",
      "| 5 Seasons|quinnjohnson/79593|2011|   0.0|Tennessee Titans|\n",
      "| 5 Seasons|quinnjohnson/79593|2011|  null|  Denver Broncos|\n",
      "+----------+------------------+----+------+----------------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# Print out the first few rows to see if the dataframe looks reasonable\n",
    "best_tackles_sdf.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "LTukZvW9Pj04"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 10/10 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# 10 points\n",
    "grader.grade(test_case_id = 'tackle_ratio', answer = best_tackles_sdf.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCcU96OwwqT1"
   },
   "source": [
    "\n",
    "### Step 2.2: The Tremendous Tackles\n",
    "\n",
    "**TODO**: Your task is to answer the following questions: \n",
    "\n",
    "1. Which team has the player which the best ratio? Save their ID to the variable `highest_tackle_ratio_person`.\n",
    "2. Which team has the total highest ratio? Save this to the variable `highest_tackle_ratio_team`\n",
    "3. For each player who has played for more than a year, what is the percentage change in their ratio from the first year they played to the last year they played? Save this df in `percentage_change_sdf`, and make sure to order by ID. This DF should just contain two columns- percentage change and ID, named as follows `Percentage Change, ID`: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "vLTmvD9WNQH3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "best_tackles_sdf.createOrReplaceTempView(\"best_tackles\")\n",
    "#TODO: highest_tackle_ratio_person\n",
    "query1 = ''' Select ID \n",
    "             From best_tackles \n",
    "             ORDER BY Ratio DESC\n",
    "             LIMIT 1\n",
    "           '''\n",
    "\n",
    "highest_tackle_ratio_person= spark.sql(query1).toPandas()['ID'].to_list()[0]\n",
    "\n",
    "#TODO: highest_tackle_ratio_team\n",
    "query2 = ''' Select TEAM \n",
    "             From best_tackles \n",
    "             ORDER BY Ratio DESC\n",
    "             LIMIT 1\n",
    "           '''\n",
    "\n",
    "highest_tackle_ratio_team= spark.sql(query2).toPandas()['TEAM'].to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "a6Ot6rFKUrH3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 2/2 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n",
      "+--------------------+--------------+--------+----+-------------------+------------+-------------+------------+----------------+-----+-------+---------------+----+------------+---------+-------------+------------------+---+--------------------+---------+----------------+--------------+-------------------+----------+---------------+--------------+--------------------+--------------+------+--------+------------+------------+\n",
      "|           Player Id|          Name|Position|Year|               Team|Games Played|Total Tackles|Solo Tackles|Assisted Tackles|Sacks|Safties|Passes Defended|Ints|Ints for TDs|Int Yards|Yards Per Int|Longest Int Return|Age|         Birth Place| Birthday|         College|Current Status|       Current Team|Experience|Height (inches)|   High School|High School Location|          Name|Number|Position|Weight (lbs)|Years Played|\n",
      "+--------------------+--------------+--------+----+-------------------+------------+-------------+------------+----------------+-----+-------+---------------+----+------------+---------+-------------+------------------+---+--------------------+---------+----------------+--------------+-------------------+----------+---------------+--------------+--------------------+--------------+------+--------+------------+------------+\n",
      "|  quinnjohnson/79593|Johnson, Quinn|        |2013|   Tennessee Titans|           4|            0|          --|              --|   --|     --|             --|  --|          --|       --|            0|                --| 30|    New Orleans , LA|9/30/1986|             LSU|       Retired|                   | 5 Seasons|             73|              |                    |Johnson, Quinn|  null|        |         255| 2009 - 2013|\n",
      "|  quinnjohnson/79593|Johnson, Quinn|        |2012|   Tennessee Titans|          16|            1|           1|               0|    0|     --|              0|  --|          --|       --|            0|                --| 30|    New Orleans , LA|9/30/1986|             LSU|       Retired|                   | 5 Seasons|             73|              |                    |Johnson, Quinn|  null|        |         255| 2009 - 2013|\n",
      "|  quinnjohnson/79593|Johnson, Quinn|        |2011|   Tennessee Titans|           4|            0|          --|              --|   --|     --|             --|  --|          --|       --|            0|                --| 30|    New Orleans , LA|9/30/1986|             LSU|       Retired|                   | 5 Seasons|             73|              |                    |Johnson, Quinn|  null|        |         255| 2009 - 2013|\n",
      "|  quinnjohnson/79593|Johnson, Quinn|        |2011|     Denver Broncos|           0|            0|          --|              --|   --|     --|             --|  --|          --|       --|            0|                --| 30|    New Orleans , LA|9/30/1986|             LSU|       Retired|                   | 5 Seasons|             73|              |                    |Johnson, Quinn|  null|        |         255| 2009 - 2013|\n",
      "|  quinnjohnson/79593|Johnson, Quinn|        |2010|  Green Bay Packers|          11|            4|           4|               0|    0|     --|              0|  --|          --|       --|            0|                --| 30|    New Orleans , LA|9/30/1986|             LSU|       Retired|                   | 5 Seasons|             73|              |                    |Johnson, Quinn|  null|        |         255| 2009 - 2013|\n",
      "|  quinnjohnson/79593|Johnson, Quinn|        |2009|  Green Bay Packers|           9|            0|          --|              --|   --|     --|             --|  --|          --|       --|            0|                --| 30|    New Orleans , LA|9/30/1986|             LSU|       Retired|                   | 5 Seasons|             73|              |                    |Johnson, Quinn|  null|        |         255| 2009 - 2013|\n",
      "|  l.t.walton/2552444|  Walton, L.T.|      DE|2016|Pittsburgh Steelers|          10|            8|           6|               2|    0|     --|              1|  --|          --|       --|            0|                --| 25|        Detroit , MI|3/31/1992|Central Michigan|        Active|Pittsburgh Steelers|3rd season|             77|Clintondale HS|Clinton Twp.,Maco...|  Walton, L.T.|    96|      DE|         305|            |\n",
      "|  l.t.walton/2552444|  Walton, L.T.|      DE|2015|Pittsburgh Steelers|           6|            0|          --|              --|   --|     --|             --|  --|          --|       --|            0|                --| 25|        Detroit , MI|3/31/1992|Central Michigan|        Active|Pittsburgh Steelers|3rd season|             77|Clintondale HS|Clinton Twp.,Maco...|  Walton, L.T.|    96|      DE|         305|            |\n",
      "|montaereagor/2502600|Reagor, Montae|        |2007|Philadelphia Eagles|           7|            2|           2|               0|    1|     --|              0|  --|          --|       --|            0|                --| 39|     Waxahachie , TX|6/29/1977|      Texas Tech|       Retired|                   |10 Seasons|             75|              |                    |Reagor, Montae|  null|        |         285| 1999 - 2008|\n",
      "|montaereagor/2502600|Reagor, Montae|        |2006| Indianapolis Colts|           5|           10|           8|               2|    1|     --|              1|  --|          --|       --|            0|                --| 39|     Waxahachie , TX|6/29/1977|      Texas Tech|       Retired|                   |10 Seasons|             75|              |                    |Reagor, Montae|  null|        |         285| 1999 - 2008|\n",
      "|montaereagor/2502600|Reagor, Montae|        |2005| Indianapolis Colts|          13|           36|          27|               9|  5.5|      0|              0|  --|          --|       --|            0|                --| 39|     Waxahachie , TX|6/29/1977|      Texas Tech|       Retired|                   |10 Seasons|             75|              |                    |Reagor, Montae|  null|        |         285| 1999 - 2008|\n",
      "|montaereagor/2502600|Reagor, Montae|        |2004| Indianapolis Colts|          16|           41|          34|               7|    5|     --|              2|  --|          --|       --|            0|                --| 39|     Waxahachie , TX|6/29/1977|      Texas Tech|       Retired|                   |10 Seasons|             75|              |                    |Reagor, Montae|  null|        |         285| 1999 - 2008|\n",
      "|montaereagor/2502600|Reagor, Montae|        |2003| Indianapolis Colts|          13|           25|          17|               8|  0.5|     --|              1|  --|          --|       --|            0|                --| 39|     Waxahachie , TX|6/29/1977|      Texas Tech|       Retired|                   |10 Seasons|             75|              |                    |Reagor, Montae|  null|        |         285| 1999 - 2008|\n",
      "|montaereagor/2502600|Reagor, Montae|        |2002|     Denver Broncos|          15|           19|          11|               8|    1|     --|              0|   1|           0|       31|           31|                31| 39|     Waxahachie , TX|6/29/1977|      Texas Tech|       Retired|                   |10 Seasons|             75|              |                    |Reagor, Montae|  null|        |         285| 1999 - 2008|\n",
      "|montaereagor/2502600|Reagor, Montae|        |2001|     Denver Broncos|           8|            9|           7|               2|    1|     --|              1|  --|          --|       --|            0|                --| 39|     Waxahachie , TX|6/29/1977|      Texas Tech|       Retired|                   |10 Seasons|             75|              |                    |Reagor, Montae|  null|        |         285| 1999 - 2008|\n",
      "|   joetafoya/2504656|   Tafoya, Joe|        |2007|  Arizona Cardinals|          13|           20|          18|               2|    2|     --|              1|  --|          --|       --|            0|                --| 38|San Luis Obispo C...| 9/6/1978|         Arizona|       Retired|                   | 7 Seasons|             76|              |                    |   Tafoya, Joe|  null|        |         258| 2001 - 2008|\n",
      "|   joetafoya/2504656|   Tafoya, Joe|        |2006|   Seattle Seahawks|          13|           12|           8|               4|    0|     --|              1|  --|          --|       --|            0|                --| 38|San Luis Obispo C...| 9/6/1978|         Arizona|       Retired|                   | 7 Seasons|             76|              |                    |   Tafoya, Joe|  null|        |         258| 2001 - 2008|\n",
      "|   joetafoya/2504656|   Tafoya, Joe|        |2005|   Seattle Seahawks|          15|           29|          19|              10|    1|     --|              1|  --|          --|       --|            0|                --| 38|San Luis Obispo C...| 9/6/1978|         Arizona|       Retired|                   | 7 Seasons|             76|              |                    |   Tafoya, Joe|  null|        |         258| 2001 - 2008|\n",
      "|   joetafoya/2504656|   Tafoya, Joe|        |2003|      Chicago Bears|          16|           17|          13|               4|    0|     --|              0|  --|          --|       --|            0|                --| 38|San Luis Obispo C...| 9/6/1978|         Arizona|       Retired|                   | 7 Seasons|             76|              |                    |   Tafoya, Joe|  null|        |         258| 2001 - 2008|\n",
      "|   joetafoya/2504656|   Tafoya, Joe|        |2002|      Chicago Bears|          14|           14|          10|               4|  0.5|     --|              0|  --|          --|       --|            0|                --| 38|San Luis Obispo C...| 9/6/1978|         Arizona|       Retired|                   | 7 Seasons|             76|              |                    |   Tafoya, Joe|  null|        |         258| 2001 - 2008|\n",
      "+--------------------+--------------+--------+----+-------------------+------------+-------------+------------+----------------+-----+-------+---------------+----+------------+---------+-------------+------------------+---+--------------------+---------+----------------+--------------+-------------------+----------+---------------+--------------+--------------------+--------------+------+--------+------------+------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# 2 points\n",
    "grader.grade(test_case_id = 'best_player', answer = highest_tackle_ratio_person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "YZa4prSwQUJQ",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 2/2 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# 2 points\n",
    "grader.grade(test_case_id = 'best_team', answer = highest_tackle_ratio_team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "id": "NyVBZzE9Zg-G"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------------------+\n",
      "|                ID_F|Year_F|            Ratio_F|\n",
      "+--------------------+------+-------------------+\n",
      "|a'shawnrobinson/2...|  2016|              1.875|\n",
      "|   a.j.bouye/2541162|  2013| 0.3333333333333333|\n",
      "|     a.j.edds/496921|  2011| 1.3333333333333333|\n",
      "|     a.j.edds/496921|  2011|                0.5|\n",
      "|  a.j.feeley/2504566|  2001|                0.0|\n",
      "| a.j.francis/2541707|  2015|                1.0|\n",
      "| a.j.francis/2541707|  2015|                0.5|\n",
      "|   a.j.green/2495450|  2011|0.13333333333333333|\n",
      "|a.j.jefferson/494275|  2010|                0.5|\n",
      "|   a.j.klein/2539982|  2013|             1.3125|\n",
      "| a.j.schable/2506734|  2006| 0.9090909090909091|\n",
      "| a.j.tarpley/2553605|  2015| 0.5714285714285714|\n",
      "|aaronbeasley/2499587|  2001|               3.25|\n",
      "|aaronburbridge/25...|  2016|             0.8125|\n",
      "| aaroncolvin/2543501|  2014|                4.5|\n",
      "| aarondobson/2539256|  2013|0.08333333333333333|\n",
      "| aarondonald/2543485|  2014|                3.0|\n",
      "| aaronelling/2504975|  2003|               0.25|\n",
      "|aaronfrancisco/25...|  2005| 0.6363636363636364|\n",
      "|  aaronglenn/2500813|  2001| 2.5384615384615383|\n",
      "+--------------------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+------+------------------+\n",
      "|                ID_l|Year_l|           Ratio_l|\n",
      "+--------------------+------+------------------+\n",
      "|a'shawnrobinson/2...|  2016|             1.875|\n",
      "|   a.j.bouye/2541162|  2016|               4.2|\n",
      "|     a.j.edds/496921|  2014|               0.0|\n",
      "|     a.j.edds/496921|  2014|              0.25|\n",
      "|  a.j.feeley/2504566|  2011|               0.0|\n",
      "| a.j.francis/2541707|  2016|              null|\n",
      "|   a.j.green/2495450|  2016|               0.0|\n",
      "|a.j.jefferson/494275|  2013|               0.5|\n",
      "| a.j.jenkins/2534544|  1990|              null|\n",
      "| a.j.johnson/2501405|  1995|              null|\n",
      "|   a.j.klein/2539982|  2016| 2.066666666666667|\n",
      "| a.j.schable/2506734|  2006|0.9090909090909091|\n",
      "| a.j.tarpley/2553605|  2015|0.5714285714285714|\n",
      "|aaronbeasley/2499587|  2004|1.5714285714285714|\n",
      "|aaronburbridge/25...|  2016|            0.8125|\n",
      "| aaroncolvin/2543501|  2016|               2.2|\n",
      "| aarondobson/2539256|  2016|              null|\n",
      "| aarondonald/2543485|  2016|            2.9375|\n",
      "| aaronelling/2504975|  2005|0.1111111111111111|\n",
      "|aaronfrancisco/25...|  2010| 4.916666666666667|\n",
      "+--------------------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-------------------+\n",
      "|                  ID|  Percentage Change|\n",
      "+--------------------+-------------------+\n",
      "|   a.j.bouye/2541162| 1160.0000000000002|\n",
      "|     a.j.edds/496921| -99.99999999999999|\n",
      "|     a.j.edds/496921|             -100.0|\n",
      "|     a.j.edds/496921|             -81.25|\n",
      "|     a.j.edds/496921|              -50.0|\n",
      "|  a.j.feeley/2504566|               null|\n",
      "| a.j.francis/2541707|               null|\n",
      "| a.j.francis/2541707|               null|\n",
      "|   a.j.green/2495450|             -100.0|\n",
      "|a.j.jefferson/494275|                0.0|\n",
      "|   a.j.klein/2539982| 57.460317460317476|\n",
      "|aaronbeasley/2499587| -51.64835164835165|\n",
      "| aaroncolvin/2543501| -51.11111111111111|\n",
      "| aarondobson/2539256|               null|\n",
      "| aarondonald/2543485|-2.0833333333333335|\n",
      "| aaronelling/2504975| -55.55555555555556|\n",
      "|aaronfrancisco/25...|  672.6190476190476|\n",
      "|  aaronglenn/2500813|-21.212121212121207|\n",
      "|aaronkampman/2505138|             -100.0|\n",
      "|  aaronlynch/2543650| 29.192546583850937|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "#TODO Create % change table and save it as percentage_change_sdf\n",
    "defensive_stats_sdf.createOrReplaceTempView(\"defensive_stats\")\n",
    "\n",
    "query1 = '''WITH t1 as(\n",
    "            Select `Player Id` as ID, `Name`,`Year`, `Team`, `Games Played` as GP,`Total Tackles` as TT\n",
    "            From defensive_stats\n",
    "            )\n",
    "            SELECT ID, Name, Year, Team, TT/GP as Ratio\n",
    "            FROM t1\n",
    "\n",
    "           '''\n",
    "table1_sdf= spark.sql(query1)\n",
    "table1_sdf.createOrReplaceTempView(\"table1\")\n",
    "\n",
    "\n",
    "\n",
    "query5 = '''WITH t1 as(\n",
    "            Select `Player Id` as ID, `Name`,`Year`, `Team`, `Games Played` as GP,`Total Tackles` as TT\n",
    "            From defensive_stats\n",
    "            )\n",
    "            SELECT ID, Name, Year, Team, TT/GP as Ratio\n",
    "            FROM t1\n",
    "            WHERE TT/GP != 0 or TT/GP IS NOT NULL \n",
    "           '''\n",
    "table5_sdf= spark.sql(query5)\n",
    "table5_sdf.createOrReplaceTempView(\"table5\")\n",
    "\n",
    "#last\n",
    "query2 = '''\n",
    "            SELECT ID as ID_l,Year as Year_l, Ratio as Ratio_l FROM table1 \n",
    "            INNER JOIN(\n",
    "            Select ID as ID_l, MAX(Year) as Year_l\n",
    "            From table1\n",
    "            GROUP BY ID_l)\n",
    "            On ID = ID_l and Year = Year_l\n",
    "            ORDER BY ID_l\n",
    "           '''\n",
    "table2_sdf= spark.sql(query2)\n",
    "\n",
    "#first\n",
    "query3 = '''WITH t2 as(\n",
    "            SELECT ID as ID_F,Year as Year_F, Ratio as Ratio_F \n",
    "            FROM table5\n",
    "            INNER JOIN(\n",
    "            Select ID as ID_F, MIN(Year) as Year_F\n",
    "            From table5\n",
    "            GROUP BY ID_F)\n",
    "            On ID = ID_F and Year = Year_F)\n",
    "            SELECT *\n",
    "            FROM t2\n",
    "            ORDER BY ID_F\n",
    "           '''\n",
    "table3_sdf= spark.sql(query3)\n",
    "table2_sdf.createOrReplaceTempView(\"table2\")\n",
    "table3_sdf.createOrReplaceTempView(\"table3\")\n",
    "\n",
    "query4 = '''With t1 as(\n",
    "            SELECT ID_l as ID, 100*(Ratio_l-Ratio_F )/Ratio_F as  `Percentage Change`, Year_l-Year_F as Year_Diff\n",
    "            FROM table2\n",
    "            FULL OUTER JOIN(\n",
    "            SELECT * FROM table3)\n",
    "            On ID_l = ID_F)\n",
    "            SELECT `ID`,`Percentage Change` \n",
    "            FROM t1\n",
    "            WHERE Year_Diff>=1\n",
    "            ORDER BY ID\n",
    "           '''\n",
    "percentage_change_sdf= spark.sql(query4)\n",
    "\n",
    "table3_sdf.show()\n",
    "table2_sdf.show()\n",
    "percentage_change_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "id": "SzCb93YH7BcB"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                  ID|  Percentage Change|\n",
      "+--------------------+-------------------+\n",
      "|   a.j.bouye/2541162| 1160.0000000000002|\n",
      "|     a.j.edds/496921|             -100.0|\n",
      "|     a.j.edds/496921| -99.99999999999999|\n",
      "|     a.j.edds/496921|             -81.25|\n",
      "|     a.j.edds/496921|              -50.0|\n",
      "|  a.j.feeley/2504566|               null|\n",
      "| a.j.francis/2541707|               null|\n",
      "| a.j.francis/2541707|               null|\n",
      "|   a.j.green/2495450|             -100.0|\n",
      "|a.j.jefferson/494275|                0.0|\n",
      "|   a.j.klein/2539982| 57.460317460317476|\n",
      "|aaronbeasley/2499587| -51.64835164835165|\n",
      "| aaroncolvin/2543501| -51.11111111111111|\n",
      "| aarondobson/2539256|               null|\n",
      "| aarondonald/2543485|-2.0833333333333335|\n",
      "| aaronelling/2504975| -55.55555555555556|\n",
      "|aaronfrancisco/25...|  672.6190476190476|\n",
      "|  aaronglenn/2500813|-21.212121212121207|\n",
      "|aaronkampman/2505138|             -100.0|\n",
      "|  aaronlynch/2543650| 29.192546583850937|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Let's visualize the results\n",
    "percentage_change_sdf.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b151a7e6b1594577909d31143227bb1b"
     ]
    },
    "id": "W3MZT21Qp5Fk",
    "outputId": "ca1870d4-daaf-4ff9-c1e8-5f8a6cdabf99"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "## AUTOGRADER Step 2.2: ##\n",
    "percentage_change_sdf.createOrReplaceTempView(\"test_2_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "id": "S8i_bc6bU93w"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 12/12 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# 12 points\n",
    "grader.grade(test_case_id = 'percentage_change', answer = spark.sql(\"SELECT * FROM test_2_2 ORDER BY ID\").toPandas())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZIfJGqDqKzX"
   },
   "source": [
    "## 2.3 The Blessed Break\n",
    "\n",
    "That last section was hard. And it's gonna get harder. Take a break. Sit back and relax for a minute. Listen to some music. \n",
    "\n",
    "**TODO**: In the cell below fill out the boolean variable `whatd_you_think` with `True` if you liked it or `False` if you didn't. You will be graded on your response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "WkA0_E2485jy"
   },
   "outputs": [],
   "source": [
    "whatd_you_think = 'True' # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "4lS-XkWl2deZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 1/1 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# 1 point\n",
    "grader.grade(test_case_id = 'tunes', answer = whatd_you_think)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkF2RfLSXO0u"
   },
   "source": [
    "## Step 3: Formatting the Time Series Data\n",
    "\n",
    "\n",
    "Our overarching goal is to create an annual time series model that will use the highest tackle ratio per year for each team to predict the highest tackle ratio for that team over the next season. The model is trained on a set of observations. Each observation contains the team name, and the highest tackle ratios for each year from 2001 to 2016. \n",
    "\n",
    "```\n",
    "+----+-----+----------+---------+----------+---\n",
    "|Team |2001 |...  |2008 |2009 |   ...   |2016 |\n",
    "+----+-----+----------+---------+----------+---\n",
    "|TEN  |...  |...  |...  |...  |   ...   |...  |\n",
    "|SF   |...  |...  |...  |...  |   ...   |...  |\n",
    "|...  |...  |...  |...  |...  |   ...   |...  |\n",
    "+----+-----+----------+---------+----------+---\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzEa-C36_x45"
   },
   "source": [
    "\n",
    "### Step 3.1: The Annual Arrangement\n",
    "\n",
    "Your first task is to create the time series table, i.e. the `2001` through `2016` columns. This will involve reshaping `best_tackles_sdf`. Currently, `best_tackles_sdf` has columns `ID`, `Name`, `Team`, `Ratio` and `Year`. We want to group the rows together based on common `Team` and create new columns for the Ratio corresponding to each year.\n",
    "\n",
    "**TODO**: Create an sdf called `raw_tackles_time_series_sdf` that has for a single team, the highest tackle ratio in 2001 through 2016. It is ok if for a given team you don't have a given year. However, ensure that for a given team, each year column has an entry, i.e. if no Ratio value is present for the year, the entry for that year column should be `0`. The format of the sdf is shown below: \n",
    "```\n",
    "+----+-----+----------+---------+----------+----------+-------\n",
    "|Team                |2001 |...  |2008 |2009 |   ...   |2016 |\n",
    "+----+-----+----------+---------+----------+----------+-------\n",
    "|Tennessee Titans    |...  |...  |...  |...  |   ...   |...  |\n",
    "|San Francisco 49ers |...  |...  |...  |...  |   ...   |...  |\n",
    "|...                 |...  |...  |...  |...  |   ...   |...  |\n",
    "+----+-----+----------+---------+----------+----------+-------\n",
    "```\n",
    "_Hint_: This is a **fiddly and somewhat difficult** question. The tricky part is creating the additional columns of annual ratios, specifically when there are missing years. \n",
    "\n",
    "We suggest you look into `CASE` and `WHEN` statements in the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html), and use these to **either** fill in a number for column (if appropriate) or put in a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "id": "btp2wboHqg2J"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# TODO: Create and save raw_tackles_time_series_sdf\n",
    "\n",
    "# CASE() statements are SQL's equivalent of if else statements. WHEN a CASE is\n",
    "# true THEN we define a function. ELSE we do another function and then END the\n",
    "# statement.\n",
    "\n",
    "# The query is a GROUP BY. We group data based on the same team, as in the\n",
    "# previous step. We then do a CASE statement. This will seperate out the sets of\n",
    "# data corresponding to the same year using the 'Year' volumn in the WHEN clause.\n",
    "# If we have a piece of data, it will be the Ratio value for a given team in \n",
    "# a given year and we will save it with a corresponding column name. If there is\n",
    "# no piece of data here, as per the question, we need to add a 0. This is the\n",
    "# ELSE clause. Lastly, we do a MAX() to find the final Ratio.\n",
    "best_tackles_sdf.createOrReplaceTempView(\"best_tackles\")\n",
    "query1 = '''\n",
    "             WITH t2 AS(\n",
    "             WITH t1 AS(Select Team, Year,MAX(Ratio) as Max_Ratio\n",
    "             From best_tackles\n",
    "             GROUP BY Team,Year)\n",
    "             SELECT Team,\n",
    "                 CASE WHEN Year == 2001 THEN Max_Ratio ELSE 0 end as `2001`,\n",
    "                 CASE WHEN Year == 2002 THEN Max_Ratio ELSE 0 end as `2002`,\n",
    "                 CASE WHEN Year == 2003 THEN Max_Ratio ELSE 0 end as `2003`,\n",
    "                 CASE WHEN Year == 2004 THEN Max_Ratio ELSE 0 end as `2004`,\n",
    "                 CASE WHEN Year == 2005 THEN Max_Ratio ELSE 0 end as `2005`,\n",
    "                 CASE WHEN Year == 2006 THEN Max_Ratio ELSE 0 end as `2006`,\n",
    "                 CASE WHEN Year == 2007 THEN Max_Ratio ELSE 0 end as `2007`,\n",
    "                 CASE WHEN Year == 2008 THEN Max_Ratio ELSE 0 end as `2008`,\n",
    "                 CASE WHEN Year == 2009 THEN Max_Ratio ELSE 0 end as `2009`,\n",
    "                 CASE WHEN Year == 2010 THEN Max_Ratio ELSE 0 end as `2010`,\n",
    "                 CASE WHEN Year == 2011 THEN Max_Ratio ELSE 0 end as `2011`,\n",
    "                 CASE WHEN Year == 2012 THEN Max_Ratio ELSE 0 end as `2012`,\n",
    "                 CASE WHEN Year == 2013 THEN Max_Ratio ELSE 0 end as `2013`,\n",
    "                 CASE WHEN Year == 2014 THEN Max_Ratio ELSE 0 end as `2014`,\n",
    "                 CASE WHEN Year == 2015 THEN Max_Ratio ELSE 0 end as `2015`,\n",
    "                 CASE WHEN Year == 2016 THEN Max_Ratio ELSE 0 end as `2016`\n",
    "             FROM t1)\n",
    "             SELECT Team, MAX(`2001`) as `2001`,MAX(`2002`) as `2002`,MAX(`2003`) as `2003`,MAX(`2004`) as `2004`,MAX(`2005`) as `2005`\n",
    "             ,MAX(`2006`) as `2006`,MAX(`2007`) as `2007`,MAX(`2008`) as `2008`,MAX(`2009`) as `2009`,MAX(`2010`) as `2010`,\n",
    "             MAX(`2011`) as `2011`,MAX(`2012`) as `2012`,MAX(`2013`) as `2013`,MAX(`2014`) as `2014`,MAX(`2015`) as `2015`,MAX(`2016`) as `2016`\n",
    "             FROM t2 \n",
    "             GROUP BY Team\n",
    "             \n",
    "           '''\n",
    "raw_tackles_time_series_sdf= spark.sql(query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "0-rrd5zHan4k"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-----------------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+-----------------+-----------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "|                Team|             2001|             2002|              2003|             2004|             2005|             2006|             2007|             2008|              2009|             2010|             2011|             2012|             2013|              2014|             2015|             2016|\n",
      "+--------------------+-----------------+-----------------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+-----------------+-----------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "| Washington Redskins|7.071428571428571|           6.1875| 6.866666666666666|              7.0|            6.125|           6.9375|              8.0|           8.3125|             8.875|9.444444444444445|           10.375|           8.6875|           7.1875| 8.384615384615385|7.333333333333333|             7.75|\n",
      "|    Tennessee Titans|6.833333333333333|              6.0|            5.3125|6.733333333333333|              5.0|           6.6875|            7.625|5.461538461538462|            7.5625|             10.0|6.866666666666666|              6.5|              6.5|               7.0|              6.8|              6.5|\n",
      "|Tampa Bay Buccaneers|              5.0|           7.0625|7.2727272727272725|6.933333333333334|           8.0625|8.666666666666666|5.533333333333333|5.363636363636363|               5.5|            5.125|             5.25|           8.6875|           9.0625|10.428571428571429|           9.1875|           9.0625|\n",
      "|      St. Louis Rams|             7.25|             7.25| 6.076923076923077|           5.9375|            5.875|            5.375|5.357142857142857|              6.5| 6.166666666666667|6.533333333333333|             5.75|            4.875|           7.3125|            6.9375|             10.5|              0.0|\n",
      "|    Seattle Seahawks|             6.25|7.888888888888889|               6.0|           5.5625|              6.5|            7.625|           6.8125|6.266666666666667|               6.5|              5.5|6.466666666666667|             8.75|8.571428571428571| 9.454545454545455|              7.6|          10.4375|\n",
      "| San Francisco 49ers|6.214285714285714|            5.875|             5.875|             5.75|              8.2|             4.75|              6.5|              6.0|             5.875|              5.0|           8.9375|             9.25|           9.0625|            5.4375|            9.625|             8.75|\n",
      "|  San Diego Chargers|7.642857142857143|              8.0|           10.0625|            9.375|              9.5|           8.8125|           5.3125|           7.9375|6.3076923076923075|              6.0|              6.0|6.416666666666667|           7.1875|             7.125|6.916666666666667|6.583333333333333|\n",
      "| Pittsburgh Steelers|5.333333333333333|5.857142857142857|            8.8125|            5.875|              8.5|            7.875|            6.125|           8.3125|             6.375|           8.4375|           5.8125|           7.0625|              8.0|              8.25|           7.4375|            7.125|\n",
      "| Philadelphia Eagles|           6.1875|             5.75|               5.6|              5.5|           6.6875|           5.8125|            6.375|           4.8125|               6.0|4.333333333333333|6.166666666666667|6.642857142857143|7.066666666666666| 6.916666666666667|           6.8125|            6.375|\n",
      "|     Oakland Raiders|              5.5|           7.8125|              8.25|5.142857142857143|5.833333333333333|           6.6875|              4.6|              6.0|              7.75|              6.5|           6.8125|           6.8125|              7.0|              10.0|            7.625|6.866666666666666|\n",
      "+--------------------+-----------------+-----------------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+-----------------+-----------------+-----------------+-----------------+------------------+-----------------+-----------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "raw_tackles_time_series_sdf.createOrReplaceTempView(\"test_3_1\")\n",
    "test_3_1_sdf = spark.sql(\"SELECT * FROM test_3_1 ORDER BY Team DESC LIMIT 10\")\n",
    "test_3_1_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "id": "NluLbb4HX5AS"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 14/14 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# 14 points\n",
    "grader.grade(test_case_id = 'arrangement', answer = test_3_1_sdf.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZRuAU0XI84l"
   },
   "source": [
    "### Step 3.2 The Clairvoyant Cleaning\n",
    "\n",
    "We now want to format the `Team` names to an appropriate abbreviation. The formatting will consist of 2 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DdnoFkP7mxz"
   },
   "source": [
    "#### Step 3.2.1 The Ubiquitous UDF\n",
    "\n",
    "The teams in our dataset are defined by their full names. We think that it would be *cleaner* to have names represented by using abbreviations. Often times when using Spark, there may not be a built-in SQL function that can do the operation we desired. Instead, we can create one on our own with a user-defined function (udf).\n",
    "\n",
    "A udf is defined as a normal Python function and then registered to be used as a Spark SQL function. Your task is to create a udf, `NAME_TO_ABBV()` that will convert the `Team` field in `raw_tackles_time_series_sdf` to their appropriate abbreviations. This will be done using the provided `name_to_abbv_dict` dictionary. We are only interested in the teams in that dictionary.\n",
    "\n",
    "**TODO**: Fill out the function `name_to_abbv()` below. Then use `spark.udf.register()` to register it as a SQL function. The command is provided. ***You do not need to edit it***. Note, we have defined the udf as returning `StringType()`. Ensure that your function returns this. You must also deal with any potential `null` cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "id": "P4cJWZsr8iNC"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function name_to_abbv at 0x7ff18bf7b9e0>"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Dictionary linking stock ticker symbols to their names\n",
    "name_to_abbv_dict = {'Tennessee Titans': 'TEN',\n",
    "                     'Denver Broncos': 'DEN',\n",
    "                     'Green Bay Packers': 'GBP',\n",
    "                     'Pittsburgh Steelers': 'PIT',\n",
    "                     'Chicago Bears': 'CHI',\n",
    "                     'Philadelphia Eagles': 'PHI',\n",
    "                     'Indianapolis Colts': 'IND',\n",
    "                     'Arizona Cardinals': 'ARI',\n",
    "                     'Seattle Seahawks': 'SEA',\n",
    "                     'Baltimore Ravens': 'BAR',\n",
    "                     'Carolina Panthers': 'CAR',\n",
    "                     'Kansas City Chiefs': 'KAN',\n",
    "                     'Oakland Raiders': 'OAK',\n",
    "                     'St. Louis Rams': 'SLR',\n",
    "                     'Atlanta Falcons': 'ATL',\n",
    "                     'New Orleans Saints': 'NOS',\n",
    "                     'San Francisco 49ers': 'SFF',\n",
    "                     'New England Patriots': 'NEP',\n",
    "                     'Buffalo Bills': 'BUF',\n",
    "                     'Los Angeles Rams': 'LAR',\n",
    "                     'Dallas Cowboys': 'DAL',\n",
    "                     'Minnesota Vikings': 'MIN',\n",
    "                     'Detroit Lions': 'DET',\n",
    "                     'Washington Redskins': 'WAS',\n",
    "                     'Jacksonville Jaguars': 'JAC',\n",
    "                     'New York Giants': 'NYG',\n",
    "                     'Tampa Bay Buccaneers': 'TBB',\n",
    "                     'Cleveland Browns': 'CLE',\n",
    "                     'Houston Texans': 'HOT',\n",
    "                     'Los Angeles Raiders': 'LOS',\n",
    "                     'Miami Dolphins': 'MIA',\n",
    "                     'Houston Oilers': 'HOU',\n",
    "                     'Cincinnati Bengals': 'CIN',\n",
    "                     'San Diego Chargers': 'SDC',\n",
    "                     'Tennessee Oilers': 'TEO',\n",
    "                     'New York Jets': 'NYJ',\n",
    "                     'Phoenix Cardinals': 'PHO',\n",
    "                     'St. Louis Cardinals': 'SLC',\n",
    "                     'Baltimore Colts': 'BAC'}\n",
    "\n",
    "# TODO: Fill out name_to_abbv() and register it as a udf.\n",
    "\n",
    "# In UDFs we have to cover all possible output cases, or else the function will\n",
    "# crash. Specifically, this means we need to handle the case when \"name\" is\n",
    "# not in \"name_to_abbv_dict\". We use a try and except statement to return null\n",
    "# for this case.\n",
    "\n",
    "def name_to_abbv(name):\n",
    "    try: \n",
    "        a = name_to_abbv_dict[name]\n",
    "        return a \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Register udf as a SQL function. DO NOT EDIT\n",
    "spark.udf.register(\"NAME_TO_ABBV\", name_to_abbv, StringType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSM0mzPRaKKr"
   },
   "source": [
    "Submit a tuple to the autograder for the ticker value of San Franscisco 49ers and Washington Commanders. If the name value in the table, set it to a string equal to \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "id": "NkdrFilOtKqx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "## AUTOGRADER Step 3.2.1: ##\n",
    "\n",
    "to_submit = ((str(name_to_abbv(\"San Francisco 49ers\")),str(name_to_abbv(\"Washington Commanders\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "id": "BA95n1lpZ4yy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 2/2 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# 2 points\n",
    "grader.grade(test_case_id = 'clairvoyant', answer = to_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9YOYO9L-_GS"
   },
   "source": [
    "#### Step 3.2.2: The Clean Conversion\n",
    "\n",
    "With our new `NAME_TO_ABBV()` function we will begin to wrangle `raw_tackles_time_series_sdf`.\n",
    "\n",
    "**TODO**: Create an sdf called `tackles_time_series_sdf` as follows. Convert all the team names in `raw_tackles_time_series_sdf` to the appropriate abbreviations and save it as `Team`. Drop any team abbreviations that do not appear in `name_to_abbv_dict`. Using .dropna() is acceptable instead of IS NOT NULL. The final df should be in the format shown below:\n",
    "\n",
    "```\n",
    "+----+-----+----------+---------+----------+----------+--\n",
    "|Team |2000 |2001 |   ...   |2008 |2009 |   ...   |2016 |\n",
    "+----+-----+----------+---------+----------+----------+--\n",
    "|TEN  |...  |...  |   ...   |...  |...  |   ...   |...  |\n",
    "|SFF  |...  |...  |   ...   |...  |...  |   ...   |...  |\n",
    "|...  |...  |...  |   ...   |...  |...  |   ...   |...  |\n",
    "+----+-----+----------+---------+----------+----------+--\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "id": "RuiitnWlBYJ7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# Format the \"Team\" column using our UDF, NAME_TO_ABBV.\n",
    "# TODO: Create and save tackles_time_series_sdf\n",
    "\n",
    "\n",
    "best_tackles_sdf.createOrReplaceTempView(\"best_tackles\")\n",
    "query2 = '''\n",
    "             WITH t2 AS(\n",
    "             WITH t1 AS(Select Team, Year,MAX(Ratio) as Max_Ratio\n",
    "             From best_tackles\n",
    "             GROUP BY Team,Year)\n",
    "             SELECT Team,\n",
    "                 CASE WHEN Year == 2001 THEN Max_Ratio ELSE 0 end as `2001`,\n",
    "                 CASE WHEN Year == 2002 THEN Max_Ratio ELSE 0 end as `2002`,\n",
    "                 CASE WHEN Year == 2003 THEN Max_Ratio ELSE 0 end as `2003`,\n",
    "                 CASE WHEN Year == 2004 THEN Max_Ratio ELSE 0 end as `2004`,\n",
    "                 CASE WHEN Year == 2005 THEN Max_Ratio ELSE 0 end as `2005`,\n",
    "                 CASE WHEN Year == 2006 THEN Max_Ratio ELSE 0 end as `2006`,\n",
    "                 CASE WHEN Year == 2007 THEN Max_Ratio ELSE 0 end as `2007`,\n",
    "                 CASE WHEN Year == 2008 THEN Max_Ratio ELSE 0 end as `2008`,\n",
    "                 CASE WHEN Year == 2009 THEN Max_Ratio ELSE 0 end as `2009`,\n",
    "                 CASE WHEN Year == 2010 THEN Max_Ratio ELSE 0 end as `2010`,\n",
    "                 CASE WHEN Year == 2011 THEN Max_Ratio ELSE 0 end as `2011`,\n",
    "                 CASE WHEN Year == 2012 THEN Max_Ratio ELSE 0 end as `2012`,\n",
    "                 CASE WHEN Year == 2013 THEN Max_Ratio ELSE 0 end as `2013`,\n",
    "                 CASE WHEN Year == 2014 THEN Max_Ratio ELSE 0 end as `2014`,\n",
    "                 CASE WHEN Year == 2015 THEN Max_Ratio ELSE 0 end as `2015`,\n",
    "                 CASE WHEN Year == 2016 THEN Max_Ratio ELSE 0 end as `2016`\n",
    "             FROM t1)\n",
    "             SELECT name_to_abbv(Team) as Team, MAX(`2001`) as `2001`,MAX(`2002`) as `2002`,MAX(`2003`) as `2003`,MAX(`2004`) as `2004`,MAX(`2005`) as `2005`\n",
    "             ,MAX(`2006`) as `2006`,MAX(`2007`) as `2007`,MAX(`2008`) as `2008`,MAX(`2009`) as `2009`,MAX(`2010`) as `2010`,\n",
    "             MAX(`2011`) as `2011`,MAX(`2012`) as `2012`,MAX(`2013`) as `2013`,MAX(`2014`) as `2014`,MAX(`2015`) as `2015`,MAX(`2016`) as `2016`\n",
    "             FROM t2 \n",
    "             GROUP BY Team\n",
    "             \n",
    "           '''\n",
    "tackles_time_series_sdf= spark.sql(query2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "id": "qBm-D6FXtdv6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "## AUTOGRADER Step 3.2.2: ##\n",
    "\n",
    "tackles_time_series_sdf.createOrReplaceTempView(\"test_3_2_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "id": "ghKZ3HHOaXIq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 10/10 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook."
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# 10 points\n",
    "grader.grade(test_case_id = 'conversion', answer = spark.sql(\"SELECT * FROM test_3_2_2 ORDER BY Team LIMIT 10\").toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvDlOG74JPl7"
   },
   "source": [
    "And there you go! Now we have formatted a simple time series dataset for the highest tackle ratios of all teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnwHS5-kPtVl"
   },
   "source": [
    "## Step 4: The Super... Bacon?\n",
    "\n",
    "Let's introduce a fun little concept called the Bacon Number! The Bacon number of an actor or actress is the number of degrees of separation they have from actor Kevin Bacon, as defined by the game known as Six Degrees of Kevin Bacon. For example, Kevin Bacon's Bacon number is 0. If an actor works in a movie with Kevin Bacon, the actor's Bacon number is 1. If an actor works with an actor who worked with Kevin Bacon in a movie, the first actor's Bacon number is 2, and so forth.\n",
    "\n",
    "How do we implement this for NFL though? Let's use a dataset specifically based on the history of the Super Bowl, and find a number we're calling \"The Super Bacon.\" We define this number as follows: if team A has beaten team B, and team B has beaten team C, then the super bacon of C with respect to A will be 2.\n",
    "\n",
    "Now to calculate this number, we'll use the concepts of graphs and BFS!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPOLIiHpYr86"
   },
   "source": [
    "*For this section, we will be using Pyspark instead of Spark on the EMR cluster. The syntax remains the same, except you don't need to use the `%%spark` call at the top of each code cell.\n",
    "Run the setup cells for Pyspark below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "id": "VGEag3L-3cp8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/25 21:23:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "appName = \"PySpark\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LK-JxKe0xBvS"
   },
   "source": [
    "### 4.1 “Traversing” a Graph\n",
    "\n",
    "Before we jump to finding the Super Bacons for the teams, let's review how BFS works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "id": "GT6WJwy5vK_G"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpanImv_x2l9"
   },
   "source": [
    "\n",
    "#### 4.1.1 Intro to Distributed Breadth-First Search\n",
    "\n",
    "\n",
    "To start off, we will be implementing a graph traversal algorithm known as Breadth First Search. It works in a way that's equivalent to how a stain spreads on a white t-shirt. Take a look at the graph below:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src = \"https://imgur.com/WU3AUwg.png\" width= \"600\" align =\"center\"/>\n",
    "\n",
    "* Consider starting BFS from point A (green). This is considered the starting frontier/singular origin node.\n",
    "* The first round of BFS would involve finding all the nodes directly reachable from A, namely B-F (blue circles). These blue nodes make up the next frontier at depth 1 away from our starting node A.\n",
    "* The second round would then be identifying the red nodes which are the neighbors of the blue nodes. Now, the red nodes all belong to a frontier 2 depth away from A. Note that node A is also a neighbor of a blue node. However, since it has already been visited, it does not get added to this frontier.\n",
    "\n",
    "This process continues until all the nodes in the graph have been visited.\n",
    "If you would like to learn more about BFS, we highly suggest looking [here](https://www.tutorialspoint.com/data_structures_algorithms/breadth_first_traversal.html).\n",
    "\n",
    "\n",
    "We will now be implementing **spark_bfs(G, N, d)**, our spark flavor of BFS that takes a graph **G**, a set of origin nodes **N**, and a max depth **d**.\n",
    "\n",
    "In order to write a successful BFS function, you are going to need to figure out \n",
    "1. how to keep track of nodes that we have visited\n",
    "2. how to properly find all the nodes at the next depth\n",
    "3. how to avoid cycles and ensure that we do not constantly loop through the same edges (take a look at J-K in the graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9QtEcHJyFTK"
   },
   "source": [
    "#### BFS vs. DFS Animation [ADDITIONAL INFO] \n",
    "Here's a helpful visualzation to understand the difference between depth and breadth first search! (Source: Wikimedia Commons)\n",
    "\n",
    "#### **BFS**\n",
    "![BFS](https://upload.wikimedia.org/wikipedia/commons/5/5d/Breadth-First-Search-Algorithm.gif)\n",
    "#### **DFS**\n",
    "![DFS](https://upload.wikimedia.org/wikipedia/commons/7/7f/Depth-First-Search.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wp6kU4RWyQw4"
   },
   "source": [
    "#### 4.1.2 Implement One Traversal\n",
    "\n",
    "To break down this process, let's think about how we would implement a single traversal of the graph. That is given the green node in the graph above, how are we going to get the blue nodes?\n",
    "\n",
    "\n",
    "Consider the simple graph below **which is different from the graph in the image above**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "id": "VZV6fQIxyaww"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|from_node|to_node|\n",
      "+---------+-------+\n",
      "|        A|      B|\n",
      "|        A|      C|\n",
      "|        A|      D|\n",
      "|        C|      F|\n",
      "|        F|      A|\n",
      "|        B|      G|\n",
      "|        G|      H|\n",
      "|        D|      E|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simple = [('A', 'B'),\n",
    "         ('A', 'C'),\n",
    "         ('A', 'D'),\n",
    "         ('C', 'F'),\n",
    "         ('F', 'A'),\n",
    "         ('B', 'G'),\n",
    "         ('G', 'H'),\n",
    "         ('D', 'E')]\n",
    "\n",
    "simple_dict = {'from_node': ['A', 'A', 'A', 'C', 'F', 'B', 'G', 'D'],\n",
    "       'to_node': ['B', 'C', 'D', 'F', 'A', 'G', 'H', 'E']}\n",
    "\n",
    "simple_graph_df = pd.DataFrame.from_dict(simple_dict)\n",
    "simple_graph_sdf = spark.createDataFrame(simple_graph_df)\n",
    "simple_graph_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQuo6wH-yr9D"
   },
   "source": [
    "As you can see, each row of this dataframe represents an edge between two nodes Here, we are looking at a **directed graph**, which means that A-->B  does not represent the same edge as B-->A.\n",
    "\n",
    "Let's define our starting node as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "id": "jmaJlqRuzEEv"
   },
   "outputs": [],
   "source": [
    "smallOrig = [{'node': 'A'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6hLLqHdzHmy"
   },
   "source": [
    "Then, bfs with graph G, starting from smallOrig to depth 1, or  **spark_bfs(G, smallOrig, 1)** would output as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "MSWF2e-YzIud"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|node|distance|\n",
      "+----+--------+\n",
      "|   B|       1|\n",
      "|   D|       1|\n",
      "|   C|       1|\n",
      "|   A|       0|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simple_1_round_dict = {'node': ['B', 'D', 'C', 'A'],\n",
    "       'distance': [1, 1, 1, 0]}\n",
    "simple_1_round_bfs_df = pd.DataFrame.from_dict(simple_1_round_dict)\n",
    "simple_1_round_bfs_sdf = spark.createDataFrame(simple_1_round_bfs_df)\n",
    "simple_1_round_bfs_sdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eL9Jx1BLzZME"
   },
   "source": [
    "As you can see, this dataframe logs each node with its corresponding distance away from A. Moreover, we also know that these nodes are **visited**. \n",
    "\n",
    "Hopefully, you can see how we can use our original graph and this new information to find the nodes at depth two. \n",
    "\n",
    "This is exactly what we will try to accomplish with **spark_bfs_1_round(visited_nodes)** which will ultimately be the inner function of **spark_bfs** that we use to perform exactly one traversal of a graph.\n",
    "\n",
    "**TODO**: Write **spark_bfs_1_round(visted_nodes)** that takes the currently dataframe of visited_nodes, performs one round of BFS, and returns an updated visited nodes dataframe. You should assume that a temporary sdf G already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "id": "vZcC22Bfzi6F"
   },
   "outputs": [],
   "source": [
    "def spark_bfs_1_round(visited_nodes):\n",
    "  \"\"\"\n",
    "  :param visited_nodes: dataframe with columns node and distance\n",
    "  :return: dataframe of updated visuted nodes, with columns node and distance\n",
    "  \"\"\"\n",
    "  # TODO: Complete this function to implement 1 round of BFS\n",
    "  next_sdf =  visited_nodes\n",
    "  visited_sdf = visited_nodes\n",
    "  #second round\n",
    "  edge_sdf = next_sdf.join(simple_graph_sdf, (next_sdf['node'] == simple_graph_sdf['from_node']),'inner').cache()\n",
    "  next_sdf = edge_sdf.select(edge_sdf['to_node'].alias('node'))\n",
    "  from_node = edge_sdf.select(edge_sdf['from_node'].alias('node'))\n",
    "  next_sdf = next_sdf.union(from_node).drop_duplicates(['node']).cache()\n",
    "  next_sdf = next_sdf.join(visited_sdf, visited_sdf['node'] == next_sdf['node'], 'leftanti')\n",
    "  visited_sdf = visited_sdf.union(next_sdf.withColumn('distance',F.lit(2))).drop_duplicates().cache()\n",
    "  return visited_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDY1lkNtz3zX"
   },
   "source": [
    "Now, run the inner function on **simple_1_round_bfs_sdf** (i.e. result of 1 round of BFS on the simple graph) and store the results in **simple_bfs_result**. This is ultimately what the output of BFS to depth 2 should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "id": "TxNEr82Z0EYY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/25 21:28:58 WARN CacheManager: Asked to cache already cached data.\n",
      "22/10/25 21:28:58 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run spark_bfs_1_round on simple_1_round_bfs_sdf\n",
    "simple_bfs_test_sdf = spark_bfs_1_round(simple_1_round_bfs_sdf)\n",
    "\n",
    "#simple_1_round_bfs_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJMBU6I50Ls6"
   },
   "source": [
    "Convert this result to Pandas, sorted by the node (ascending), and submit it to the autograder.\n",
    "\n",
    "**HINT:** Make sure distance is formatted as a number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "id": "DltsLEUz0PUg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  node  distance\n",
       "6    A         0\n",
       "2    B         1\n",
       "3    C         1\n",
       "5    D         1\n",
       "1    E         2\n",
       "4    F         2\n",
       "0    G         2"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Convert simple_bfs_result to Pandas sorted by node\n",
    "simple_bfs_test = simple_bfs_test_sdf.toPandas().sort_values(by = 'node',ascending = True)\n",
    "simple_bfs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "id": "rKpFy0tU0R8t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 6/6 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# 6 points\n",
    "grader.grade(test_case_id = 'checksimpleBFS', answer = simple_bfs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUZugRR20T4F"
   },
   "source": [
    "#### 4.1.3 Full BFS Implementation\n",
    "\n",
    "Now, we will fully implement **spark_bfs**. This function should iteratively call your implemented version of **spark_bfs_1_round** and ultimately return the output of this function at **max_depth**.\n",
    "\n",
    "You are also responsible for initializing the starting dataframe, that is converting the list of origin nodes into a spark dataframe with the nodes logged at distance 0.\n",
    "\n",
    "Consider the following: \n",
    "\n",
    "```python\n",
    "schema = StructType([\n",
    "            StructField(\"node\", StringType(), True)\n",
    "        ])\n",
    "my_sdf = spark.createDataFrame(origins, schema)\n",
    "```\n",
    "\n",
    "The schema ultimately specifies the structure of the Spark DataFrame with a string `node` column. It then calls **spark.createDataFrame** to map this schema to the **origins** nodes. Also, you are responsible for ensuring that a view of your graph is available within this function. (Note: you will also need to add in a distance column)\n",
    "\n",
    "**TODO:** implement **spark_bfs(G,origins,max_depth)** and run on **teams_graph_sdf** initalized in 4.3. Note: you may want to run tests on the **simple_graph** example as the `teams_graph_sdf` will take quite some time to run.\n",
    "\n",
    "These imports might be useful: \n",
    "`from pyspark.sql.types import StructType, StructField, StringType, IntegerType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "id": "2OpEOcsT1Vwu"
   },
   "outputs": [],
   "source": [
    "# TODO: iterative search over directed graph\n",
    "# Worth 5 points directly, but will be needed later\n",
    "def spark_bfs(G, origins, max_depth):\n",
    "  \"\"\" runs distributed BFS to a specified max depth\n",
    "\n",
    "  :param G: graph dataframe from 4.3\n",
    "  :param origins: list of origin nodes stored as {\"node\": nodeValue}\n",
    "  :param max_depth: integer value of max depth to run BFS to\n",
    "  :return: dataframe with columns node, distance of all visited nodes\n",
    "  \"\"\"\n",
    "  next_sdf = spark.createDataFrame(origins, schema)\n",
    "  visited_sdf = spark.createDataFrame(origins, schema).withColumn('distance',F.lit(0))\n",
    "  for distance in range(max_depth):\n",
    "      edge_sdf = next_sdf.join(G, (next_sdf['node'] == G['from_node']),'inner').cache()\n",
    "      next_sdf = edge_sdf.select(edge_sdf['to_node'].alias('node'))\n",
    "      from_node = edge_sdf.select(edge_sdf['from_node'].alias('node'))\n",
    "      next_sdf = next_sdf.union(from_node).drop_duplicates(['node']).cache()\n",
    "      next_sdf = next_sdf.join(visited_sdf, visited_sdf['node'] == next_sdf['node'], 'leftanti')\n",
    "      visited_sdf = visited_sdf.union(next_sdf.withColumn('distance',F.lit(distance+1))).drop_duplicates().cache()\n",
    "  return visited_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"node\",StringType(),True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZsmVVlE1r_v"
   },
   "source": [
    "Test that this function works on the simple example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "id": "Ideb3M_c1t4N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|node|distance|\n",
      "+----+--------+\n",
      "|   G|       2|\n",
      "|   F|       2|\n",
      "|   E|       2|\n",
      "|   A|       0|\n",
      "|   C|       1|\n",
      "|   B|       1|\n",
      "|   D|       1|\n",
      "|   H|       3|\n",
      "+----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "simple_bfs_iterative_result = spark_bfs(simple_graph_sdf, smallOrig, 3)\n",
    "simple_bfs_iterative_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6mLUZvAuwDK"
   },
   "source": [
    "### Step 4.2: History of the Super Bowl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUJ49VklGB92"
   },
   "source": [
    "Time to look at the Super Bowl history dataset. Here we have data for all Super Bowls from 1967 to 2020. Let's load the data to **superbowl_sdf** and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "id": "ih_KUCTAvvWo"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addFile(\"https://storage.googleapis.com/penn-cis5450/superbowl.csv\")\n",
    "superbowl_sdf = spark.read.csv(SparkFiles.get(\"superbowl.csv\"), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "id": "FMy3cSNTvvnF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------------+----------+--------------------+---------+---------------+--------------------+---------------+----------+\n",
      "|       Date|          SB|              Winner|Winner Pts|               Loser|Loser Pts|            MVP|             Stadium|           City|     State|\n",
      "+-----------+------------+--------------------+----------+--------------------+---------+---------------+--------------------+---------------+----------+\n",
      "| Feb 2 2020|    LIV (54)|  Kansas City Chiefs|        31| San Francisco 49ers|       20|Patrick Mahomes|   Hard Rock Stadium|  Miami Gardens|   Florida|\n",
      "| Feb 3 2019|   LIII (53)|New England Patriots|        13|    Los Angeles Rams|        3| Julian Edelman|Mercedes-Benz Sta...|        Atlanta|   Georgia|\n",
      "| Feb 4 2018|    LII (52)| Philadelphia Eagles|        41|New England Patriots|       33|     Nick Foles|   U.S. Bank Stadium|    Minneapolis| Minnesota|\n",
      "| Feb 5 2017|     LI (51)|New England Patriots|        34|     Atlanta Falcons|       28|      Tom Brady|         NRG Stadium|        Houston|     Texas|\n",
      "| Feb 7 2016|          50|      Denver Broncos|        24|   Carolina Panthers|       10|     Von Miller|      Levi's Stadium|    Santa Clara|California|\n",
      "| Feb 1 2015|   XLIX (49)|New England Patriots|        28|    Seattle Seahawks|       24|      Tom Brady|University of Pho...|       Glendale|   Arizona|\n",
      "| Feb 2 2014| XLVIII (48)|    Seattle Seahawks|        43|      Denver Broncos|        8|  Malcolm Smith|     MetLife Stadium|East Rutherford|New Jersey|\n",
      "| Feb 3 2013|  XLVII (47)|    Baltimore Ravens|        34| San Francisco 49ers|       31|     Joe Flacco|Mercedes-Benz Sup...|    New Orleans| Louisiana|\n",
      "| Feb 5 2012|   XLVI (46)|     New York Giants|        21|New England Patriots|       17|    Eli Manning|   Lucas Oil Stadium|   Indianapolis|   Indiana|\n",
      "| Feb 6 2011|    XLV (45)|   Green Bay Packers|        31| Pittsburgh Steelers|       25|  Aaron Rodgers|     Cowboys Stadium|      Arlington|     Texas|\n",
      "| Feb 7 2010|   XLIV (44)|  New Orleans Saints|        31|  Indianapolis Colts|       17|     Drew Brees|    Sun Life Stadium|  Miami Gardens|   Florida|\n",
      "| Feb 1 2009|  XLIII (43)| Pittsburgh Steelers|        27|   Arizona Cardinals|       23|Santonio Holmes|Raymond James Sta...|          Tampa|   Florida|\n",
      "| Feb 3 2008|   XLII (42)|     New York Giants|        17|New England Patriots|       14|    Eli Manning|University of Pho...|       Glendale|   Arizona|\n",
      "| Feb 4 2007|    XLI (41)|  Indianapolis Colts|        29|       Chicago Bears|       17| Peyton Manning|     Dolphin Stadium|  Miami Gardens|   Florida|\n",
      "| Feb 5 2006|     XL (40)| Pittsburgh Steelers|        21|    Seattle Seahawks|       10|     Hines Ward|          Ford Field|        Detroit|  Michigan|\n",
      "| Feb 6 2005|  XXXIX (39)|New England Patriots|        24| Philadelphia Eagles|       21|   Deion Branch|      Alltel Stadium|   Jacksonville|   Florida|\n",
      "| Feb 1 2004|XXXVIII (38)|New England Patriots|        32|   Carolina Panthers|       29|      Tom Brady|     Reliant Stadium|        Houston|     Texas|\n",
      "|Jan 26 2003| XXXVII (37)|Tampa Bay Buccaneers|        48|     Oakland Raiders|       21| Dexter Jackson|    Qualcomm Stadium|      San Diego|California|\n",
      "| Feb 3 2002|  XXXVI (36)|New England Patriots|        20|      St. Louis Rams|       17|      Tom Brady| Louisiana Superdome|    New Orleans| Louisiana|\n",
      "|Jan 28 2001|   XXXV (35)|    Baltimore Ravens|        34|     New York Giants|        7|     Ray Lewis+|Raymond James Sta...|          Tampa|   Florida|\n",
      "+-----------+------------+--------------------+----------+--------------------+---------+---------------+--------------------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superbowl_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUxD2OD0GeYE"
   },
   "source": [
    "Do you know which teams have been the most successful during this period? Let's find out!\n",
    "\n",
    "**TODO:** Calculate the number of times each team appears in the *Winner* column, and store this **count_sdf**. This should have two columns: *Winner* and *win_count*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "id": "LwDGFymQv66L"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "count_df =superbowl_sdf.cube(\"Winner\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qpjvw2j3I9Nt"
   },
   "source": [
    "Convert the dataframe to pandas, naming it `count_df` and pass it to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "id": "_p7uRMfran4n"
   },
   "outputs": [],
   "source": [
    "# TODO: create count_df \n",
    "count_df = count_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "id": "0y-AIOxyH3s-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 3/3 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# 3 points\n",
    "grader.grade(test_case_id = 'checkSuperBowlCount', answer = count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDQHXODMI35m"
   },
   "source": [
    "Now that we have the Super Bowl dataset, let's convert it to a graph sdf just like the one we had in section 4.1 (P.S. it's not as hard as it sounds).\n",
    "\n",
    "**TODO:** Create **teams_graph_sdf** that has the columns *from_node* and *to_node*. from_node has all the entries from the *Winner* column and to_node has all the entries from the *Loser* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "id": "N85KxevSv4ei"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    " #spark.createDataFrame(simple_graph_df)\n",
    "superbowl_sdf.createOrReplaceTempView(\"superbowl\")\n",
    "query1 = ''' Select Winner as from_node, Loser as to_node\n",
    "             From superbowl\n",
    "           '''\n",
    "    \n",
    "teams_graph_sdf = spark.sql(query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "id": "sQrBW1FcMNEP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 2/2 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# 2 points\n",
    "grader.grade(test_case_id = 'checkSuperBowlGraph', answer = teams_graph_sdf.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5hCeztRMe2D"
   },
   "source": [
    "We have our graph ready! All that's left is running full BFS on it find the Super Bacons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYMybeI82fgC"
   },
   "source": [
    "### Step 4.3: The Super Bacon Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQyp_EjeM_16"
   },
   "source": [
    "In the previous section, we found out that the New England Patriots and the Pittsburgh Steelers have been the most successful teams in the 54 years. So let's find out the Super Bacons of teams with respect to the New England Patriots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4iQD5nJOrJy"
   },
   "source": [
    "First, we'll create an origin node using the New England Patriots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "id": "YolRMKEl2xJG"
   },
   "outputs": [],
   "source": [
    "orig = [{'node': 'New England Patriots'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rw_lX0nPPoYT"
   },
   "source": [
    "Now, run the **spark_bfs()** function on **teams_graph_sdf** using this origin node and a max depth of 5. Store the result in **bfs_5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "id": "8VmoiLtK21Ru"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "answer_df = spark_bfs(teams_graph_sdf, orig, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 257:=================================================>   (186 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                node|distance|\n",
      "+--------------------+--------+\n",
      "|   Arizona Cardinals|       5|\n",
      "|     Atlanta Falcons|       1|\n",
      "|   Carolina Panthers|       1|\n",
      "|      Dallas Cowboys|       5|\n",
      "|      Denver Broncos|       2|\n",
      "|   Green Bay Packers|       3|\n",
      "|  Kansas City Chiefs|       4|\n",
      "|    Los Angeles Rams|       1|\n",
      "|   Minnesota Vikings|       5|\n",
      "|New England Patriots|       0|\n",
      "|     Oakland Raiders|       4|\n",
      "| Philadelphia Eagles|       1|\n",
      "| Pittsburgh Steelers|       4|\n",
      "| San Francisco 49ers|       5|\n",
      "|    Seattle Seahawks|       1|\n",
      "|      St. Louis Rams|       1|\n",
      "|    Tennessee Titans|       2|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bfs_5 = answer_df.sort(F.col('node'))\n",
    "bfs_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "id": "oCCXO6Wv4KNR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 15/15 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# 15 points\n",
    "grader.grade(test_case_id = 'checkBFS', answer = bfs_5.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dePHd9CQW-C"
   },
   "source": [
    "Voila! We're all done! One last thing, as we predicted before, you're a star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "id": "5KwTvy0wan4n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\r\n",
      "[NbConvertApp] WARNING | pattern 'homework3.ipynb' matched no files\r\n",
      "This application is used to convert notebook files (*.ipynb)\r\n",
      "        to various other formats.\r\n",
      "\r\n",
      "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\r\n",
      "\r\n",
      "Options\r\n",
      "=======\r\n",
      "The options below are convenience aliases to configurable class-options,\r\n",
      "as listed in the \"Equivalent to\" description-line of the aliases.\r\n",
      "To see all configurable class-options for some <cmd>, use:\r\n",
      "    <cmd> --help-all\r\n",
      "\r\n",
      "--debug\r\n",
      "    set log level to logging.DEBUG (maximize logging output)\r\n",
      "    Equivalent to: [--Application.log_level=10]\r\n",
      "--show-config\r\n",
      "    Show the application's configuration (human-readable format)\r\n",
      "    Equivalent to: [--Application.show_config=True]\r\n",
      "--show-config-json\r\n",
      "    Show the application's configuration (json format)\r\n",
      "    Equivalent to: [--Application.show_config_json=True]\r\n",
      "--generate-config\r\n",
      "    generate default config file\r\n",
      "    Equivalent to: [--JupyterApp.generate_config=True]\r\n",
      "-y\r\n",
      "    Answer yes to any questions instead of prompting.\r\n",
      "    Equivalent to: [--JupyterApp.answer_yes=True]\r\n",
      "--execute\r\n",
      "    Execute the notebook prior to export.\r\n",
      "    Equivalent to: [--ExecutePreprocessor.enabled=True]\r\n",
      "--allow-errors\r\n",
      "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\r\n",
      "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\r\n",
      "--stdin\r\n",
      "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\r\n",
      "    Equivalent to: [--NbConvertApp.from_stdin=True]\r\n",
      "--stdout\r\n",
      "    Write notebook output to stdout instead of files.\r\n",
      "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\r\n",
      "--inplace\r\n",
      "    Run nbconvert in place, overwriting the existing notebook (only \r\n",
      "            relevant when converting to notebook format)\r\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\r\n",
      "--clear-output\r\n",
      "    Clear output of current file and save in place, \r\n",
      "            overwriting the existing notebook.\r\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\r\n",
      "--no-prompt\r\n",
      "    Exclude input and output prompts from converted document.\r\n",
      "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\r\n",
      "--no-input\r\n",
      "    Exclude input cells and output prompts from converted document. \r\n",
      "            This mode is ideal for generating code-free reports.\r\n",
      "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\r\n",
      "--allow-chromium-download\r\n",
      "    Whether to allow downloading chromium if no suitable version is found on the system.\r\n",
      "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\r\n",
      "--disable-chromium-sandbox\r\n",
      "    Disable chromium security sandbox when converting to PDF..\r\n",
      "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\r\n",
      "--show-input\r\n",
      "    Shows code input. This is flag is only useful for dejavu users.\r\n",
      "    Equivalent to: [--TemplateExporter.exclude_input=False]\r\n",
      "--log-level=<Enum>\r\n",
      "    Set the log level by value or name.\r\n",
      "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\r\n",
      "    Default: 30\r\n",
      "    Equivalent to: [--Application.log_level]\r\n",
      "--config=<Unicode>\r\n",
      "    Full path of a config file.\r\n",
      "    Default: ''\r\n",
      "    Equivalent to: [--JupyterApp.config_file]\r\n",
      "--to=<Unicode>\r\n",
      "    The export format to be used, either one of the built-in formats\r\n",
      "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf']\r\n",
      "            or a dotted object name that represents the import path for an\r\n",
      "            ``Exporter`` class\r\n",
      "    Default: ''\r\n",
      "    Equivalent to: [--NbConvertApp.export_format]\r\n",
      "--template=<Unicode>\r\n",
      "    Name of the template to use\r\n",
      "    Default: ''\r\n",
      "    Equivalent to: [--TemplateExporter.template_name]\r\n",
      "--template-file=<Unicode>\r\n",
      "    Name of the template file to use\r\n",
      "    Default: None\r\n",
      "    Equivalent to: [--TemplateExporter.template_file]\r\n",
      "--writer=<DottedObjectName>\r\n",
      "    Writer class used to write the \r\n",
      "                                        results of the conversion\r\n",
      "    Default: 'FilesWriter'\r\n",
      "    Equivalent to: [--NbConvertApp.writer_class]\r\n",
      "--post=<DottedOrNone>\r\n",
      "    PostProcessor class used to write the\r\n",
      "                                        results of the conversion\r\n",
      "    Default: ''\r\n",
      "    Equivalent to: [--NbConvertApp.postprocessor_class]\r\n",
      "--output=<Unicode>\r\n",
      "    overwrite base name use for output files.\r\n",
      "                can only be used when converting one notebook at a time.\r\n",
      "    Default: ''\r\n",
      "    Equivalent to: [--NbConvertApp.output_base]\r\n",
      "--output-dir=<Unicode>\r\n",
      "    Directory to write output(s) to. Defaults\r\n",
      "                                  to output to the directory of each notebook. To recover\r\n",
      "                                  previous default behaviour (outputting to the current \r\n",
      "                                  working directory) use . as the flag value.\r\n",
      "    Default: ''\r\n",
      "    Equivalent to: [--FilesWriter.build_directory]\r\n",
      "--reveal-prefix=<Unicode>\r\n",
      "    The URL prefix for reveal.js (version 3.x).\r\n",
      "            This defaults to the reveal CDN, but can be any url pointing to a copy \r\n",
      "            of reveal.js. \r\n",
      "            For speaker notes to work, this must be a relative path to a local \r\n",
      "            copy of reveal.js: e.g., \"reveal.js\".\r\n",
      "            If a relative path is given, it must be a subdirectory of the\r\n",
      "            current directory (from which the server is run).\r\n",
      "            See the usage documentation\r\n",
      "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\r\n",
      "            for more details.\r\n",
      "    Default: ''\r\n",
      "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\r\n",
      "--nbformat=<Enum>\r\n",
      "    The nbformat version to write.\r\n",
      "            Use this to downgrade notebooks.\r\n",
      "    Choices: any of [1, 2, 3, 4]\r\n",
      "    Default: 4\r\n",
      "    Equivalent to: [--NotebookExporter.nbformat_version]\r\n",
      "\r\n",
      "Examples\r\n",
      "--------\r\n",
      "\r\n",
      "    The simplest way to use nbconvert is\r\n",
      "\r\n",
      "            > jupyter nbconvert mynotebook.ipynb --to html\r\n",
      "\r\n",
      "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf'].\r\n",
      "\r\n",
      "            > jupyter nbconvert --to latex mynotebook.ipynb\r\n",
      "\r\n",
      "            Both HTML and LaTeX support multiple output templates. LaTeX includes\r\n",
      "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and \r\n",
      "            'classic'. You can specify the flavor of the format used.\r\n",
      "\r\n",
      "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\r\n",
      "\r\n",
      "            You can also pipe the output to stdout, rather than a file\r\n",
      "\r\n",
      "            > jupyter nbconvert mynotebook.ipynb --stdout\r\n",
      "\r\n",
      "            PDF is generated via latex\r\n",
      "\r\n",
      "            > jupyter nbconvert mynotebook.ipynb --to pdf\r\n",
      "\r\n",
      "            You can get (and serve) a Reveal.js-powered slideshow\r\n",
      "\r\n",
      "            > jupyter nbconvert myslides.ipynb --to slides --post serve\r\n",
      "\r\n",
      "            Multiple notebooks can be given at the command line in a couple of \r\n",
      "            different ways:\r\n",
      "\r\n",
      "            > jupyter nbconvert notebook*.ipynb\r\n",
      "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\r\n",
      "\r\n",
      "            or you can specify the notebooks list in a config file, containing::\r\n",
      "\r\n",
      "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\r\n",
      "\r\n",
      "            > jupyter nbconvert --config mycfg.py\r\n",
      "\r\n",
      "To see all available configurables, use `--help-all`.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to convert homework3.ipynb to homework3.py (see submission instructions below)\n",
    "\n",
    "!jupyter nbconvert --to script 'homework3.ipynb' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHVFHPVIrvu1"
   },
   "source": [
    "# HW Submission\n",
    "\n",
    "Before you submit on Gradescope (you must submit your notebook to receive credit):\n",
    "\n",
    "\n",
    "1.   Restart and Run-All to make sure there's nothing wrong with your notebook\n",
    "2.   **Double check that you have the correct PennID (all numbers) in the autograder**. \n",
    "3. Make sure you've run all the PennGrader cells\n",
    "4. Make sure that this notebook is named `homework3.ipynb` and then run the cell above to export it to a python file (`homework3.py`). If needed, **rename** the files to make sure they are **named exactly** \"homework3.ipynb\" and \"homework3.py\" respectively and upload them to Gradescope \n",
    "\n",
    "**Let the course staff know ASAP if you have any issues submitting, but otherwise best of luck!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
